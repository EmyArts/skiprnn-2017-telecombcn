{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%% Import files\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Nans: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "folder = '../../LastGridsearch'\n",
    "\n",
    "csvs = []\n",
    "\n",
    "count_nan = 0\n",
    "for i, file in enumerate(os.listdir(folder + '/csvs')):\n",
    "    df = pd.read_csv(folder + '/csvs' + '/' + file)\n",
    "    if (df.batch_size == 64).any():\n",
    "        df.rename(columns={'Unnamed: 0' : 'epoch'}, inplace=True)\n",
    "        count_nan += df.shape[0] * df.shape[1] - np.sum(df.count())\n",
    "        df['list_index'] = len(csvs)\n",
    "        filename = file.split(\"_\")\n",
    "        # print(filename[0][-2:].isdigit())\n",
    "        if filename[0][-2:].isdigit(): df['exp'] = filename[0][-2:]\n",
    "        else: df['exp'] = filename[0][-1]\n",
    "        csvs.append(df)\n",
    "\n",
    "print(f\"Total number of Nans: {count_nan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "nan_index = []\n",
    "for i, df in enumerate(csvs):\n",
    "    nan_index.append(list(df[(df['val_acc'].isnull()) | (df['train_acc'].isnull()) |(df['train_updates'].isnull()) | (df['val_updates'].isnull())].index))\n",
    "\n",
    "print(nan_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# weird_acc = []\n",
    "# for i, df in enumerate(csvs):\n",
    "#     weird_acc.append(list(df[(df['val_acc']>1) | (df['val_acc']<0.1) | (df['train_acc']>1) | (df['train_acc']<0.1)].index))\n",
    "# # for df in csvs:\n",
    "# #     df.drop(df[(df['val_acc']>1) | (df['val_acc']<0)].index, inplace = True)\n",
    "#\n",
    "# see = [csvs[i].iloc[l[0]] for i, l in enumerate(weird_acc) if l]\n",
    "#\n",
    "# early_stopped_dfs = []\n",
    "# for i in range(len(csvs)):\n",
    "#     n = nan_index[i]\n",
    "#     v = weird_acc[i]\n",
    "#     if n and v:\n",
    "#         early_stopped_dfs.append(csvs[i].iloc[[min(min(v), min(n))]])\n",
    "#     elif n:\n",
    "#         early_stopped_dfs.append(csvs[i].iloc[[min(n)]])\n",
    "#     elif v:\n",
    "#         early_stopped_dfs.append(csvs[i].iloc[[min(v)]])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# early_stopped = pd.concat(early_stopped_dfs)\n",
    "# early_stopped.drop(columns=['val_acc', 'train_acc', 'val_updates', 'train_updates', 'early_stopping'], inplace=True)\n",
    "# print(\"Networks that stopped early\")\n",
    "# early_stopped\n",
    "# csvs[0].columns\n",
    "# csvs[2]\n",
    "# for df in csvs:\n",
    "#     print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# manual_early_stopping = {4: 36,5: 28, 12: 12, 14:27, 32:18, 35:23, 41:35, 46: 12,53:12}\n",
    "#\n",
    "# for key, val in manual_early_stopping.items():\n",
    "#     csvs[key].drop(labels = range(val, 40), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             epoch  batch_size  cost_per_sample  hidden_units  learning_rate  \\\ncount  4126.000000      4126.0       4126.00000        4126.0   4.126000e+03   \nmean     44.508483        64.0          0.00029          32.0   2.500000e-04   \nstd      26.986853         0.0          0.00041           0.0   1.084334e-19   \nmin       0.000000        64.0          0.00001          32.0   2.500000e-04   \n25%      21.000000        64.0          0.00001          32.0   2.500000e-04   \n50%      43.000000        64.0          0.00010          32.0   2.500000e-04   \n75%      66.000000        64.0          0.00010          32.0   2.500000e-04   \nmax      99.000000        64.0          0.00100          32.0   2.500000e-04   \n\n       surprisal_cost        trial      val_acc  val_updates    train_acc  \\\ncount     4126.000000  4126.000000  4126.000000  4126.000000  4126.000000   \nmean         0.292523     0.985216     0.800814   194.040822     0.817989   \nstd          0.427535     0.822428     0.067350    80.732328     0.077605   \nmin          0.000000     0.000000     0.468550     6.203726     0.484575   \n25%          0.010000     0.000000     0.790465   148.369347     0.808694   \n50%          0.100000     1.000000     0.820212   211.042923     0.838909   \n75%          1.000000     2.000000     0.840645   258.597412     0.861846   \nmax          1.000000     2.000000     0.860076  1587.473267     0.894431   \n\n       train_updates   list_index  \ncount    4126.000000  4126.000000  \nmean      203.554115    23.111246  \nstd       121.438596    13.671239  \nmin         6.159589     0.000000  \n25%       151.754044    11.000000  \n50%       211.512558    23.000000  \n75%       259.044151    35.000000  \nmax      2229.119873    46.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>batch_size</th>\n      <th>cost_per_sample</th>\n      <th>hidden_units</th>\n      <th>learning_rate</th>\n      <th>surprisal_cost</th>\n      <th>trial</th>\n      <th>val_acc</th>\n      <th>val_updates</th>\n      <th>train_acc</th>\n      <th>train_updates</th>\n      <th>list_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>4126.000000</td>\n      <td>4126.0</td>\n      <td>4126.00000</td>\n      <td>4126.0</td>\n      <td>4.126000e+03</td>\n      <td>4126.000000</td>\n      <td>4126.000000</td>\n      <td>4126.000000</td>\n      <td>4126.000000</td>\n      <td>4126.000000</td>\n      <td>4126.000000</td>\n      <td>4126.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>44.508483</td>\n      <td>64.0</td>\n      <td>0.00029</td>\n      <td>32.0</td>\n      <td>2.500000e-04</td>\n      <td>0.292523</td>\n      <td>0.985216</td>\n      <td>0.800814</td>\n      <td>194.040822</td>\n      <td>0.817989</td>\n      <td>203.554115</td>\n      <td>23.111246</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>26.986853</td>\n      <td>0.0</td>\n      <td>0.00041</td>\n      <td>0.0</td>\n      <td>1.084334e-19</td>\n      <td>0.427535</td>\n      <td>0.822428</td>\n      <td>0.067350</td>\n      <td>80.732328</td>\n      <td>0.077605</td>\n      <td>121.438596</td>\n      <td>13.671239</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>64.0</td>\n      <td>0.00001</td>\n      <td>32.0</td>\n      <td>2.500000e-04</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.468550</td>\n      <td>6.203726</td>\n      <td>0.484575</td>\n      <td>6.159589</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>21.000000</td>\n      <td>64.0</td>\n      <td>0.00001</td>\n      <td>32.0</td>\n      <td>2.500000e-04</td>\n      <td>0.010000</td>\n      <td>0.000000</td>\n      <td>0.790465</td>\n      <td>148.369347</td>\n      <td>0.808694</td>\n      <td>151.754044</td>\n      <td>11.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>43.000000</td>\n      <td>64.0</td>\n      <td>0.00010</td>\n      <td>32.0</td>\n      <td>2.500000e-04</td>\n      <td>0.100000</td>\n      <td>1.000000</td>\n      <td>0.820212</td>\n      <td>211.042923</td>\n      <td>0.838909</td>\n      <td>211.512558</td>\n      <td>23.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>66.000000</td>\n      <td>64.0</td>\n      <td>0.00010</td>\n      <td>32.0</td>\n      <td>2.500000e-04</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>0.840645</td>\n      <td>258.597412</td>\n      <td>0.861846</td>\n      <td>259.044151</td>\n      <td>35.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>99.000000</td>\n      <td>64.0</td>\n      <td>0.00100</td>\n      <td>32.0</td>\n      <td>2.500000e-04</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>0.860076</td>\n      <td>1587.473267</td>\n      <td>0.894431</td>\n      <td>2229.119873</td>\n      <td>46.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_df = pd.concat(csvs)\n",
    "big_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x19efffa50c8>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "big_df['val_acc'].hist(figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_rows = []\n",
    "best_accs = []\n",
    "for df in csvs:\n",
    "    best_accs.append(df.loc[:, 'val_acc'].max())\n",
    "    best_rows.append(df.loc[df.loc[:, 'val_acc'].argmax()].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_df = pd.DataFrame(best_rows)\n",
    "best_df = best_df.drop(columns= [\"learning_rate\", \"hidden_units\", \"batch_size\", \"early_stopping\"])\n",
    "\n",
    "original = best_df[best_df['surprisal_cost'] == 0]\n",
    "surprisal = best_df[best_df['surprisal_cost'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No surprisal\n"
     ]
    },
    {
     "data": {
      "text/plain": "    epoch  cost_per_sample  surprisal_cost  trial   val_acc  val_updates  \\\n0      78          0.00100             0.0      0  0.827925    83.491783   \n3      64          0.00001             0.0      0  0.860076   236.088135   \n7      62          0.00100             0.0      1  0.822616    78.107475   \n12     74          0.00010             0.0      1  0.850661   201.210037   \n16     76          0.00005             0.0      1  0.852464   169.999802   \n20     46          0.00001             0.0      1  0.838241   187.660858   \n25     49          0.00100             0.0      2  0.819511    84.393227   \n29     90          0.00010             0.0      2  0.848157   176.211441   \n34     90          0.00005             0.0      2  0.852664   204.058792   \n37     94          0.00001             0.0      2  0.857973   220.727570   \n41     63          0.00010             0.0      0  0.848958   185.340942   \n45     33          0.00005             0.0      0  0.844050   204.964249   \n\n    train_acc  train_updates  list_index exp  \n0    0.825254      78.086075           0   0  \n3    0.870459     220.848419           3  12  \n7    0.821448      75.205597           7  16  \n12   0.875401     181.925674          12  20  \n16   0.875534     171.122864          16  24  \n20   0.841880     186.238846          20  28  \n25   0.813435      78.617188          25  32  \n29   0.864383     161.337601          29  36  \n34   0.885751     199.139160          34  40  \n37   0.878339     216.578995          37  44  \n41   0.860911     171.910522          41   4  \n45   0.841346     186.081726          45   8  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>cost_per_sample</th>\n      <th>surprisal_cost</th>\n      <th>trial</th>\n      <th>val_acc</th>\n      <th>val_updates</th>\n      <th>train_acc</th>\n      <th>train_updates</th>\n      <th>list_index</th>\n      <th>exp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>78</td>\n      <td>0.00100</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.827925</td>\n      <td>83.491783</td>\n      <td>0.825254</td>\n      <td>78.086075</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>64</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.860076</td>\n      <td>236.088135</td>\n      <td>0.870459</td>\n      <td>220.848419</td>\n      <td>3</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>62</td>\n      <td>0.00100</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0.822616</td>\n      <td>78.107475</td>\n      <td>0.821448</td>\n      <td>75.205597</td>\n      <td>7</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>74</td>\n      <td>0.00010</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0.850661</td>\n      <td>201.210037</td>\n      <td>0.875401</td>\n      <td>181.925674</td>\n      <td>12</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>76</td>\n      <td>0.00005</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0.852464</td>\n      <td>169.999802</td>\n      <td>0.875534</td>\n      <td>171.122864</td>\n      <td>16</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>46</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0.838241</td>\n      <td>187.660858</td>\n      <td>0.841880</td>\n      <td>186.238846</td>\n      <td>20</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>49</td>\n      <td>0.00100</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0.819511</td>\n      <td>84.393227</td>\n      <td>0.813435</td>\n      <td>78.617188</td>\n      <td>25</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>90</td>\n      <td>0.00010</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0.848157</td>\n      <td>176.211441</td>\n      <td>0.864383</td>\n      <td>161.337601</td>\n      <td>29</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>90</td>\n      <td>0.00005</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0.852664</td>\n      <td>204.058792</td>\n      <td>0.885751</td>\n      <td>199.139160</td>\n      <td>34</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>94</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0.857973</td>\n      <td>220.727570</td>\n      <td>0.878339</td>\n      <td>216.578995</td>\n      <td>37</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>63</td>\n      <td>0.00010</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.848958</td>\n      <td>185.340942</td>\n      <td>0.860911</td>\n      <td>171.910522</td>\n      <td>41</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>33</td>\n      <td>0.00005</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.844050</td>\n      <td>204.964249</td>\n      <td>0.841346</td>\n      <td>186.081726</td>\n      <td>45</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"No surprisal\")\n",
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With surprisal\n"
     ]
    },
    {
     "data": {
      "text/plain": "    epoch  cost_per_sample  surprisal_cost  trial   val_acc  val_updates  \\\n1      76          0.00005            0.10      0  0.856370   257.410645   \n2      70          0.00005            0.01      0  0.857572   206.600357   \n4      82          0.00001            1.00      0  0.855268   273.492584   \n5      85          0.00001            0.10      0  0.859075   259.792664   \n6      59          0.00001            0.01      0  0.853766   236.346252   \n8      98          0.00100            1.00      1  0.852965   250.222351   \n9      64          0.00100            0.10      1  0.827925    91.686699   \n10     61          0.00100            0.01      1  0.819111    79.707634   \n11     96          0.00100            1.00      0  0.850861   251.265625   \n13     96          0.00010            1.00      1  0.856971   251.753006   \n14     82          0.00010            0.10      1  0.855869   254.620392   \n15     89          0.00010            0.01      1  0.850160   155.191711   \n17     39          0.00005            1.00      1  0.845252   261.027435   \n18     60          0.00005            0.10      1  0.852865   269.353760   \n19     65          0.00005            0.01      1  0.848458   165.132004   \n21     78          0.00001            1.00      1  0.859876   262.362671   \n22     79          0.00100            0.10      0  0.839844    92.586342   \n23     75          0.00001            0.10      1  0.855569   264.001617   \n24     45          0.00001            0.01      1  0.847055   207.858170   \n26     67          0.00100            1.00      2  0.847957   244.553986   \n27     47          0.00100            0.10      2  0.822917    93.442009   \n28     98          0.00100            0.01      2  0.826522    79.428688   \n30     98          0.00010            1.00      2  0.853966   256.555084   \n31     79          0.00010            0.10      2  0.855869   261.409149   \n32     59          0.00010            0.01      2  0.850561   170.047577   \n33     74          0.00100            0.01      0  0.820012    79.853065   \n35     90          0.00005            1.00      2  0.857272   268.264313   \n36     90          0.00005            0.01      2  0.858474   193.663666   \n38     63          0.00001            1.00      2  0.852364   271.167755   \n39     99          0.00001            0.10      2  0.854067   266.856659   \n40     80          0.00001            0.01      2  0.852163   199.411362   \n42     95          0.00010            1.00      0  0.854667   259.933197   \n43     92          0.00010            0.10      0  0.850561   252.973160   \n44     86          0.00010            0.01      0  0.856070   193.423874   \n46     72          0.00005            1.00      0  0.854567   261.984589   \n\n    train_acc  train_updates  list_index exp  \n1    0.886685     256.424011           1  10  \n2    0.863849     202.653442           2  11  \n4    0.877537     269.070862           4  13  \n5    0.882145     258.979553           5  14  \n6    0.857973     233.211212           6  15  \n8    0.862313     249.674149           8  17  \n9    0.828926      80.465942           9  18  \n10   0.824386      81.062035          10  19  \n11   0.873798     252.127411          11   1  \n13   0.879006     250.925217          13  21  \n14   0.879607     254.343155          14  22  \n15   0.873197     150.166595          15  23  \n17   0.858974     260.409912          17  25  \n18   0.863114     266.564636          18  26  \n19   0.851229     162.088470          19  27  \n21   0.882545     260.516754          21  29  \n22   0.845954      86.976364          22   2  \n23   0.868590     263.836548          23  30  \n24   0.851295     208.071243          24  31  \n26   0.852631     238.858902          26  33  \n27   0.810897      83.012817          27  34  \n28   0.834402      77.394768          28  35  \n30   0.873932     256.037659          30  37  \n31   0.871327     261.522247          31  38  \n32   0.860377     163.016220          32  39  \n33   0.822182      75.997063          33   3  \n35   0.887754     264.872742          35  41  \n36   0.877270     187.984909          36  43  \n38   0.864450     269.446106          38  45  \n39   0.876536     264.420593          39  46  \n40   0.865385     210.445374          40  47  \n42   0.889156     258.170074          42   5  \n43   0.888622     252.430695          43   6  \n44   0.879140     183.047806          44   7  \n46   0.871261     260.538269          46   9  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>cost_per_sample</th>\n      <th>surprisal_cost</th>\n      <th>trial</th>\n      <th>val_acc</th>\n      <th>val_updates</th>\n      <th>train_acc</th>\n      <th>train_updates</th>\n      <th>list_index</th>\n      <th>exp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>76</td>\n      <td>0.00005</td>\n      <td>0.10</td>\n      <td>0</td>\n      <td>0.856370</td>\n      <td>257.410645</td>\n      <td>0.886685</td>\n      <td>256.424011</td>\n      <td>1</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>70</td>\n      <td>0.00005</td>\n      <td>0.01</td>\n      <td>0</td>\n      <td>0.857572</td>\n      <td>206.600357</td>\n      <td>0.863849</td>\n      <td>202.653442</td>\n      <td>2</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>82</td>\n      <td>0.00001</td>\n      <td>1.00</td>\n      <td>0</td>\n      <td>0.855268</td>\n      <td>273.492584</td>\n      <td>0.877537</td>\n      <td>269.070862</td>\n      <td>4</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>85</td>\n      <td>0.00001</td>\n      <td>0.10</td>\n      <td>0</td>\n      <td>0.859075</td>\n      <td>259.792664</td>\n      <td>0.882145</td>\n      <td>258.979553</td>\n      <td>5</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>59</td>\n      <td>0.00001</td>\n      <td>0.01</td>\n      <td>0</td>\n      <td>0.853766</td>\n      <td>236.346252</td>\n      <td>0.857973</td>\n      <td>233.211212</td>\n      <td>6</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>98</td>\n      <td>0.00100</td>\n      <td>1.00</td>\n      <td>1</td>\n      <td>0.852965</td>\n      <td>250.222351</td>\n      <td>0.862313</td>\n      <td>249.674149</td>\n      <td>8</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>64</td>\n      <td>0.00100</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>0.827925</td>\n      <td>91.686699</td>\n      <td>0.828926</td>\n      <td>80.465942</td>\n      <td>9</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>61</td>\n      <td>0.00100</td>\n      <td>0.01</td>\n      <td>1</td>\n      <td>0.819111</td>\n      <td>79.707634</td>\n      <td>0.824386</td>\n      <td>81.062035</td>\n      <td>10</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>96</td>\n      <td>0.00100</td>\n      <td>1.00</td>\n      <td>0</td>\n      <td>0.850861</td>\n      <td>251.265625</td>\n      <td>0.873798</td>\n      <td>252.127411</td>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>96</td>\n      <td>0.00010</td>\n      <td>1.00</td>\n      <td>1</td>\n      <td>0.856971</td>\n      <td>251.753006</td>\n      <td>0.879006</td>\n      <td>250.925217</td>\n      <td>13</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>82</td>\n      <td>0.00010</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>0.855869</td>\n      <td>254.620392</td>\n      <td>0.879607</td>\n      <td>254.343155</td>\n      <td>14</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>89</td>\n      <td>0.00010</td>\n      <td>0.01</td>\n      <td>1</td>\n      <td>0.850160</td>\n      <td>155.191711</td>\n      <td>0.873197</td>\n      <td>150.166595</td>\n      <td>15</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>39</td>\n      <td>0.00005</td>\n      <td>1.00</td>\n      <td>1</td>\n      <td>0.845252</td>\n      <td>261.027435</td>\n      <td>0.858974</td>\n      <td>260.409912</td>\n      <td>17</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>60</td>\n      <td>0.00005</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>0.852865</td>\n      <td>269.353760</td>\n      <td>0.863114</td>\n      <td>266.564636</td>\n      <td>18</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>65</td>\n      <td>0.00005</td>\n      <td>0.01</td>\n      <td>1</td>\n      <td>0.848458</td>\n      <td>165.132004</td>\n      <td>0.851229</td>\n      <td>162.088470</td>\n      <td>19</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>78</td>\n      <td>0.00001</td>\n      <td>1.00</td>\n      <td>1</td>\n      <td>0.859876</td>\n      <td>262.362671</td>\n      <td>0.882545</td>\n      <td>260.516754</td>\n      <td>21</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>79</td>\n      <td>0.00100</td>\n      <td>0.10</td>\n      <td>0</td>\n      <td>0.839844</td>\n      <td>92.586342</td>\n      <td>0.845954</td>\n      <td>86.976364</td>\n      <td>22</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>75</td>\n      <td>0.00001</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>0.855569</td>\n      <td>264.001617</td>\n      <td>0.868590</td>\n      <td>263.836548</td>\n      <td>23</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>45</td>\n      <td>0.00001</td>\n      <td>0.01</td>\n      <td>1</td>\n      <td>0.847055</td>\n      <td>207.858170</td>\n      <td>0.851295</td>\n      <td>208.071243</td>\n      <td>24</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>67</td>\n      <td>0.00100</td>\n      <td>1.00</td>\n      <td>2</td>\n      <td>0.847957</td>\n      <td>244.553986</td>\n      <td>0.852631</td>\n      <td>238.858902</td>\n      <td>26</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>47</td>\n      <td>0.00100</td>\n      <td>0.10</td>\n      <td>2</td>\n      <td>0.822917</td>\n      <td>93.442009</td>\n      <td>0.810897</td>\n      <td>83.012817</td>\n      <td>27</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>98</td>\n      <td>0.00100</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>0.826522</td>\n      <td>79.428688</td>\n      <td>0.834402</td>\n      <td>77.394768</td>\n      <td>28</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>98</td>\n      <td>0.00010</td>\n      <td>1.00</td>\n      <td>2</td>\n      <td>0.853966</td>\n      <td>256.555084</td>\n      <td>0.873932</td>\n      <td>256.037659</td>\n      <td>30</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>79</td>\n      <td>0.00010</td>\n      <td>0.10</td>\n      <td>2</td>\n      <td>0.855869</td>\n      <td>261.409149</td>\n      <td>0.871327</td>\n      <td>261.522247</td>\n      <td>31</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>59</td>\n      <td>0.00010</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>0.850561</td>\n      <td>170.047577</td>\n      <td>0.860377</td>\n      <td>163.016220</td>\n      <td>32</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>74</td>\n      <td>0.00100</td>\n      <td>0.01</td>\n      <td>0</td>\n      <td>0.820012</td>\n      <td>79.853065</td>\n      <td>0.822182</td>\n      <td>75.997063</td>\n      <td>33</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>90</td>\n      <td>0.00005</td>\n      <td>1.00</td>\n      <td>2</td>\n      <td>0.857272</td>\n      <td>268.264313</td>\n      <td>0.887754</td>\n      <td>264.872742</td>\n      <td>35</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>90</td>\n      <td>0.00005</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>0.858474</td>\n      <td>193.663666</td>\n      <td>0.877270</td>\n      <td>187.984909</td>\n      <td>36</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>63</td>\n      <td>0.00001</td>\n      <td>1.00</td>\n      <td>2</td>\n      <td>0.852364</td>\n      <td>271.167755</td>\n      <td>0.864450</td>\n      <td>269.446106</td>\n      <td>38</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>99</td>\n      <td>0.00001</td>\n      <td>0.10</td>\n      <td>2</td>\n      <td>0.854067</td>\n      <td>266.856659</td>\n      <td>0.876536</td>\n      <td>264.420593</td>\n      <td>39</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>80</td>\n      <td>0.00001</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>0.852163</td>\n      <td>199.411362</td>\n      <td>0.865385</td>\n      <td>210.445374</td>\n      <td>40</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>95</td>\n      <td>0.00010</td>\n      <td>1.00</td>\n      <td>0</td>\n      <td>0.854667</td>\n      <td>259.933197</td>\n      <td>0.889156</td>\n      <td>258.170074</td>\n      <td>42</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>92</td>\n      <td>0.00010</td>\n      <td>0.10</td>\n      <td>0</td>\n      <td>0.850561</td>\n      <td>252.973160</td>\n      <td>0.888622</td>\n      <td>252.430695</td>\n      <td>43</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>86</td>\n      <td>0.00010</td>\n      <td>0.01</td>\n      <td>0</td>\n      <td>0.856070</td>\n      <td>193.423874</td>\n      <td>0.879140</td>\n      <td>183.047806</td>\n      <td>44</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>72</td>\n      <td>0.00005</td>\n      <td>1.00</td>\n      <td>0</td>\n      <td>0.854567</td>\n      <td>261.984589</td>\n      <td>0.871261</td>\n      <td>260.538269</td>\n      <td>46</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"With surprisal\")\n",
    "surprisal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with all best epochs for Validation accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": "    epoch  cost_per_sample  surprisal_cost  trial   val_acc  val_updates  \\\n45     33          0.00005            0.00      0  0.844050   204.964249   \n17     39          0.00005            1.00      1  0.845252   261.027435   \n24     45          0.00001            0.01      1  0.847055   207.858170   \n20     46          0.00001            0.00      1  0.838241   187.660858   \n27     47          0.00100            0.10      2  0.822917    93.442009   \n25     49          0.00100            0.00      2  0.819511    84.393227   \n6      59          0.00001            0.01      0  0.853766   236.346252   \n32     59          0.00010            0.01      2  0.850561   170.047577   \n18     60          0.00005            0.10      1  0.852865   269.353760   \n10     61          0.00100            0.01      1  0.819111    79.707634   \n7      62          0.00100            0.00      1  0.822616    78.107475   \n38     63          0.00001            1.00      2  0.852364   271.167755   \n41     63          0.00010            0.00      0  0.848958   185.340942   \n3      64          0.00001            0.00      0  0.860076   236.088135   \n9      64          0.00100            0.10      1  0.827925    91.686699   \n19     65          0.00005            0.01      1  0.848458   165.132004   \n26     67          0.00100            1.00      2  0.847957   244.553986   \n2      70          0.00005            0.01      0  0.857572   206.600357   \n46     72          0.00005            1.00      0  0.854567   261.984589   \n12     74          0.00010            0.00      1  0.850661   201.210037   \n33     74          0.00100            0.01      0  0.820012    79.853065   \n23     75          0.00001            0.10      1  0.855569   264.001617   \n16     76          0.00005            0.00      1  0.852464   169.999802   \n1      76          0.00005            0.10      0  0.856370   257.410645   \n0      78          0.00100            0.00      0  0.827925    83.491783   \n21     78          0.00001            1.00      1  0.859876   262.362671   \n31     79          0.00010            0.10      2  0.855869   261.409149   \n22     79          0.00100            0.10      0  0.839844    92.586342   \n40     80          0.00001            0.01      2  0.852163   199.411362   \n14     82          0.00010            0.10      1  0.855869   254.620392   \n4      82          0.00001            1.00      0  0.855268   273.492584   \n5      85          0.00001            0.10      0  0.859075   259.792664   \n44     86          0.00010            0.01      0  0.856070   193.423874   \n15     89          0.00010            0.01      1  0.850160   155.191711   \n34     90          0.00005            0.00      2  0.852664   204.058792   \n29     90          0.00010            0.00      2  0.848157   176.211441   \n35     90          0.00005            1.00      2  0.857272   268.264313   \n36     90          0.00005            0.01      2  0.858474   193.663666   \n43     92          0.00010            0.10      0  0.850561   252.973160   \n37     94          0.00001            0.00      2  0.857973   220.727570   \n42     95          0.00010            1.00      0  0.854667   259.933197   \n11     96          0.00100            1.00      0  0.850861   251.265625   \n13     96          0.00010            1.00      1  0.856971   251.753006   \n8      98          0.00100            1.00      1  0.852965   250.222351   \n30     98          0.00010            1.00      2  0.853966   256.555084   \n28     98          0.00100            0.01      2  0.826522    79.428688   \n39     99          0.00001            0.10      2  0.854067   266.856659   \n\n    train_acc  train_updates  list_index exp  \n45   0.841346     186.081726          45   8  \n17   0.858974     260.409912          17  25  \n24   0.851295     208.071243          24  31  \n20   0.841880     186.238846          20  28  \n27   0.810897      83.012817          27  34  \n25   0.813435      78.617188          25  32  \n6    0.857973     233.211212           6  15  \n32   0.860377     163.016220          32  39  \n18   0.863114     266.564636          18  26  \n10   0.824386      81.062035          10  19  \n7    0.821448      75.205597           7  16  \n38   0.864450     269.446106          38  45  \n41   0.860911     171.910522          41   4  \n3    0.870459     220.848419           3  12  \n9    0.828926      80.465942           9  18  \n19   0.851229     162.088470          19  27  \n26   0.852631     238.858902          26  33  \n2    0.863849     202.653442           2  11  \n46   0.871261     260.538269          46   9  \n12   0.875401     181.925674          12  20  \n33   0.822182      75.997063          33   3  \n23   0.868590     263.836548          23  30  \n16   0.875534     171.122864          16  24  \n1    0.886685     256.424011           1  10  \n0    0.825254      78.086075           0   0  \n21   0.882545     260.516754          21  29  \n31   0.871327     261.522247          31  38  \n22   0.845954      86.976364          22   2  \n40   0.865385     210.445374          40  47  \n14   0.879607     254.343155          14  22  \n4    0.877537     269.070862           4  13  \n5    0.882145     258.979553           5  14  \n44   0.879140     183.047806          44   7  \n15   0.873197     150.166595          15  23  \n34   0.885751     199.139160          34  40  \n29   0.864383     161.337601          29  36  \n35   0.887754     264.872742          35  41  \n36   0.877270     187.984909          36  43  \n43   0.888622     252.430695          43   6  \n37   0.878339     216.578995          37  44  \n42   0.889156     258.170074          42   5  \n11   0.873798     252.127411          11   1  \n13   0.879006     250.925217          13  21  \n8    0.862313     249.674149           8  17  \n30   0.873932     256.037659          30  37  \n28   0.834402      77.394768          28  35  \n39   0.876536     264.420593          39  46  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>cost_per_sample</th>\n      <th>surprisal_cost</th>\n      <th>trial</th>\n      <th>val_acc</th>\n      <th>val_updates</th>\n      <th>train_acc</th>\n      <th>train_updates</th>\n      <th>list_index</th>\n      <th>exp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>45</th>\n      <td>33</td>\n      <td>0.00005</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.844050</td>\n      <td>204.964249</td>\n      <td>0.841346</td>\n      <td>186.081726</td>\n      <td>45</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>39</td>\n      <td>0.00005</td>\n      <td>1.00</td>\n      <td>1</td>\n      <td>0.845252</td>\n      <td>261.027435</td>\n      <td>0.858974</td>\n      <td>260.409912</td>\n      <td>17</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>45</td>\n      <td>0.00001</td>\n      <td>0.01</td>\n      <td>1</td>\n      <td>0.847055</td>\n      <td>207.858170</td>\n      <td>0.851295</td>\n      <td>208.071243</td>\n      <td>24</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>46</td>\n      <td>0.00001</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>0.838241</td>\n      <td>187.660858</td>\n      <td>0.841880</td>\n      <td>186.238846</td>\n      <td>20</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>47</td>\n      <td>0.00100</td>\n      <td>0.10</td>\n      <td>2</td>\n      <td>0.822917</td>\n      <td>93.442009</td>\n      <td>0.810897</td>\n      <td>83.012817</td>\n      <td>27</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>49</td>\n      <td>0.00100</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>0.819511</td>\n      <td>84.393227</td>\n      <td>0.813435</td>\n      <td>78.617188</td>\n      <td>25</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>59</td>\n      <td>0.00001</td>\n      <td>0.01</td>\n      <td>0</td>\n      <td>0.853766</td>\n      <td>236.346252</td>\n      <td>0.857973</td>\n      <td>233.211212</td>\n      <td>6</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>59</td>\n      <td>0.00010</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>0.850561</td>\n      <td>170.047577</td>\n      <td>0.860377</td>\n      <td>163.016220</td>\n      <td>32</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>60</td>\n      <td>0.00005</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>0.852865</td>\n      <td>269.353760</td>\n      <td>0.863114</td>\n      <td>266.564636</td>\n      <td>18</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>61</td>\n      <td>0.00100</td>\n      <td>0.01</td>\n      <td>1</td>\n      <td>0.819111</td>\n      <td>79.707634</td>\n      <td>0.824386</td>\n      <td>81.062035</td>\n      <td>10</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>62</td>\n      <td>0.00100</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>0.822616</td>\n      <td>78.107475</td>\n      <td>0.821448</td>\n      <td>75.205597</td>\n      <td>7</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>63</td>\n      <td>0.00001</td>\n      <td>1.00</td>\n      <td>2</td>\n      <td>0.852364</td>\n      <td>271.167755</td>\n      <td>0.864450</td>\n      <td>269.446106</td>\n      <td>38</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>63</td>\n      <td>0.00010</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.848958</td>\n      <td>185.340942</td>\n      <td>0.860911</td>\n      <td>171.910522</td>\n      <td>41</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>64</td>\n      <td>0.00001</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.860076</td>\n      <td>236.088135</td>\n      <td>0.870459</td>\n      <td>220.848419</td>\n      <td>3</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>64</td>\n      <td>0.00100</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>0.827925</td>\n      <td>91.686699</td>\n      <td>0.828926</td>\n      <td>80.465942</td>\n      <td>9</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>65</td>\n      <td>0.00005</td>\n      <td>0.01</td>\n      <td>1</td>\n      <td>0.848458</td>\n      <td>165.132004</td>\n      <td>0.851229</td>\n      <td>162.088470</td>\n      <td>19</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>67</td>\n      <td>0.00100</td>\n      <td>1.00</td>\n      <td>2</td>\n      <td>0.847957</td>\n      <td>244.553986</td>\n      <td>0.852631</td>\n      <td>238.858902</td>\n      <td>26</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>70</td>\n      <td>0.00005</td>\n      <td>0.01</td>\n      <td>0</td>\n      <td>0.857572</td>\n      <td>206.600357</td>\n      <td>0.863849</td>\n      <td>202.653442</td>\n      <td>2</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>72</td>\n      <td>0.00005</td>\n      <td>1.00</td>\n      <td>0</td>\n      <td>0.854567</td>\n      <td>261.984589</td>\n      <td>0.871261</td>\n      <td>260.538269</td>\n      <td>46</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>74</td>\n      <td>0.00010</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>0.850661</td>\n      <td>201.210037</td>\n      <td>0.875401</td>\n      <td>181.925674</td>\n      <td>12</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>74</td>\n      <td>0.00100</td>\n      <td>0.01</td>\n      <td>0</td>\n      <td>0.820012</td>\n      <td>79.853065</td>\n      <td>0.822182</td>\n      <td>75.997063</td>\n      <td>33</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>75</td>\n      <td>0.00001</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>0.855569</td>\n      <td>264.001617</td>\n      <td>0.868590</td>\n      <td>263.836548</td>\n      <td>23</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>76</td>\n      <td>0.00005</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>0.852464</td>\n      <td>169.999802</td>\n      <td>0.875534</td>\n      <td>171.122864</td>\n      <td>16</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>76</td>\n      <td>0.00005</td>\n      <td>0.10</td>\n      <td>0</td>\n      <td>0.856370</td>\n      <td>257.410645</td>\n      <td>0.886685</td>\n      <td>256.424011</td>\n      <td>1</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>78</td>\n      <td>0.00100</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.827925</td>\n      <td>83.491783</td>\n      <td>0.825254</td>\n      <td>78.086075</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>78</td>\n      <td>0.00001</td>\n      <td>1.00</td>\n      <td>1</td>\n      <td>0.859876</td>\n      <td>262.362671</td>\n      <td>0.882545</td>\n      <td>260.516754</td>\n      <td>21</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>79</td>\n      <td>0.00010</td>\n      <td>0.10</td>\n      <td>2</td>\n      <td>0.855869</td>\n      <td>261.409149</td>\n      <td>0.871327</td>\n      <td>261.522247</td>\n      <td>31</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>79</td>\n      <td>0.00100</td>\n      <td>0.10</td>\n      <td>0</td>\n      <td>0.839844</td>\n      <td>92.586342</td>\n      <td>0.845954</td>\n      <td>86.976364</td>\n      <td>22</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>80</td>\n      <td>0.00001</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>0.852163</td>\n      <td>199.411362</td>\n      <td>0.865385</td>\n      <td>210.445374</td>\n      <td>40</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>82</td>\n      <td>0.00010</td>\n      <td>0.10</td>\n      <td>1</td>\n      <td>0.855869</td>\n      <td>254.620392</td>\n      <td>0.879607</td>\n      <td>254.343155</td>\n      <td>14</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>82</td>\n      <td>0.00001</td>\n      <td>1.00</td>\n      <td>0</td>\n      <td>0.855268</td>\n      <td>273.492584</td>\n      <td>0.877537</td>\n      <td>269.070862</td>\n      <td>4</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>85</td>\n      <td>0.00001</td>\n      <td>0.10</td>\n      <td>0</td>\n      <td>0.859075</td>\n      <td>259.792664</td>\n      <td>0.882145</td>\n      <td>258.979553</td>\n      <td>5</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>86</td>\n      <td>0.00010</td>\n      <td>0.01</td>\n      <td>0</td>\n      <td>0.856070</td>\n      <td>193.423874</td>\n      <td>0.879140</td>\n      <td>183.047806</td>\n      <td>44</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>89</td>\n      <td>0.00010</td>\n      <td>0.01</td>\n      <td>1</td>\n      <td>0.850160</td>\n      <td>155.191711</td>\n      <td>0.873197</td>\n      <td>150.166595</td>\n      <td>15</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>90</td>\n      <td>0.00005</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>0.852664</td>\n      <td>204.058792</td>\n      <td>0.885751</td>\n      <td>199.139160</td>\n      <td>34</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>90</td>\n      <td>0.00010</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>0.848157</td>\n      <td>176.211441</td>\n      <td>0.864383</td>\n      <td>161.337601</td>\n      <td>29</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>90</td>\n      <td>0.00005</td>\n      <td>1.00</td>\n      <td>2</td>\n      <td>0.857272</td>\n      <td>268.264313</td>\n      <td>0.887754</td>\n      <td>264.872742</td>\n      <td>35</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>90</td>\n      <td>0.00005</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>0.858474</td>\n      <td>193.663666</td>\n      <td>0.877270</td>\n      <td>187.984909</td>\n      <td>36</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>92</td>\n      <td>0.00010</td>\n      <td>0.10</td>\n      <td>0</td>\n      <td>0.850561</td>\n      <td>252.973160</td>\n      <td>0.888622</td>\n      <td>252.430695</td>\n      <td>43</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>94</td>\n      <td>0.00001</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>0.857973</td>\n      <td>220.727570</td>\n      <td>0.878339</td>\n      <td>216.578995</td>\n      <td>37</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>95</td>\n      <td>0.00010</td>\n      <td>1.00</td>\n      <td>0</td>\n      <td>0.854667</td>\n      <td>259.933197</td>\n      <td>0.889156</td>\n      <td>258.170074</td>\n      <td>42</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>96</td>\n      <td>0.00100</td>\n      <td>1.00</td>\n      <td>0</td>\n      <td>0.850861</td>\n      <td>251.265625</td>\n      <td>0.873798</td>\n      <td>252.127411</td>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>96</td>\n      <td>0.00010</td>\n      <td>1.00</td>\n      <td>1</td>\n      <td>0.856971</td>\n      <td>251.753006</td>\n      <td>0.879006</td>\n      <td>250.925217</td>\n      <td>13</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>98</td>\n      <td>0.00100</td>\n      <td>1.00</td>\n      <td>1</td>\n      <td>0.852965</td>\n      <td>250.222351</td>\n      <td>0.862313</td>\n      <td>249.674149</td>\n      <td>8</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>98</td>\n      <td>0.00010</td>\n      <td>1.00</td>\n      <td>2</td>\n      <td>0.853966</td>\n      <td>256.555084</td>\n      <td>0.873932</td>\n      <td>256.037659</td>\n      <td>30</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>98</td>\n      <td>0.00100</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>0.826522</td>\n      <td>79.428688</td>\n      <td>0.834402</td>\n      <td>77.394768</td>\n      <td>28</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>99</td>\n      <td>0.00001</td>\n      <td>0.10</td>\n      <td>2</td>\n      <td>0.854067</td>\n      <td>266.856659</td>\n      <td>0.876536</td>\n      <td>264.420593</td>\n      <td>39</td>\n      <td>46</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataframe with all best epochs for Validation accuracy\")\n",
    "best_df.sort_values(by='epoch')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with one per trials\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                epoch  trial   val_acc  val_updates  \\\ncost_per_sample surprisal_cost                                        \n0.00001         0.00               64      0  0.860076   236.088135   \n                0.01               59      0  0.853766   236.346252   \n                0.10               85      0  0.859075   259.792664   \n                1.00               82      0  0.855268   273.492584   \n0.00005         0.00               76      1  0.852464   169.999802   \n                0.01               70      0  0.857572   206.600357   \n                0.10               76      0  0.856370   257.410645   \n                1.00               39      1  0.845252   261.027435   \n0.00010         0.00               74      1  0.850661   201.210037   \n                0.01               89      1  0.850160   155.191711   \n                0.10               82      1  0.855869   254.620392   \n                1.00               96      1  0.856971   251.753006   \n0.00100         0.00               78      0  0.827925    83.491783   \n                0.01               61      1  0.819111    79.707634   \n                0.10               64      1  0.827925    91.686699   \n                1.00               98      1  0.852965   250.222351   \n\n                                train_acc  train_updates  list_index exp  \ncost_per_sample surprisal_cost                                            \n0.00001         0.00             0.870459     220.848419           3  12  \n                0.01             0.857973     233.211212           6  15  \n                0.10             0.882145     258.979553           5  14  \n                1.00             0.877537     269.070862           4  13  \n0.00005         0.00             0.875534     171.122864          16  24  \n                0.01             0.863849     202.653442           2  11  \n                0.10             0.886685     256.424011           1  10  \n                1.00             0.858974     260.409912          17  25  \n0.00010         0.00             0.875401     181.925674          12  20  \n                0.01             0.873197     150.166595          15  23  \n                0.10             0.879607     254.343155          14  22  \n                1.00             0.879006     250.925217          13  21  \n0.00100         0.00             0.825254      78.086075           0   0  \n                0.01             0.824386      81.062035          10  19  \n                0.10             0.828926      80.465942           9  18  \n                1.00             0.862313     249.674149           8  17  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>epoch</th>\n      <th>trial</th>\n      <th>val_acc</th>\n      <th>val_updates</th>\n      <th>train_acc</th>\n      <th>train_updates</th>\n      <th>list_index</th>\n      <th>exp</th>\n    </tr>\n    <tr>\n      <th>cost_per_sample</th>\n      <th>surprisal_cost</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">0.00001</th>\n      <th>0.00</th>\n      <td>64</td>\n      <td>0</td>\n      <td>0.860076</td>\n      <td>236.088135</td>\n      <td>0.870459</td>\n      <td>220.848419</td>\n      <td>3</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>0.01</th>\n      <td>59</td>\n      <td>0</td>\n      <td>0.853766</td>\n      <td>236.346252</td>\n      <td>0.857973</td>\n      <td>233.211212</td>\n      <td>6</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>0.10</th>\n      <td>85</td>\n      <td>0</td>\n      <td>0.859075</td>\n      <td>259.792664</td>\n      <td>0.882145</td>\n      <td>258.979553</td>\n      <td>5</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>1.00</th>\n      <td>82</td>\n      <td>0</td>\n      <td>0.855268</td>\n      <td>273.492584</td>\n      <td>0.877537</td>\n      <td>269.070862</td>\n      <td>4</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">0.00005</th>\n      <th>0.00</th>\n      <td>76</td>\n      <td>1</td>\n      <td>0.852464</td>\n      <td>169.999802</td>\n      <td>0.875534</td>\n      <td>171.122864</td>\n      <td>16</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>0.01</th>\n      <td>70</td>\n      <td>0</td>\n      <td>0.857572</td>\n      <td>206.600357</td>\n      <td>0.863849</td>\n      <td>202.653442</td>\n      <td>2</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>0.10</th>\n      <td>76</td>\n      <td>0</td>\n      <td>0.856370</td>\n      <td>257.410645</td>\n      <td>0.886685</td>\n      <td>256.424011</td>\n      <td>1</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>1.00</th>\n      <td>39</td>\n      <td>1</td>\n      <td>0.845252</td>\n      <td>261.027435</td>\n      <td>0.858974</td>\n      <td>260.409912</td>\n      <td>17</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">0.00010</th>\n      <th>0.00</th>\n      <td>74</td>\n      <td>1</td>\n      <td>0.850661</td>\n      <td>201.210037</td>\n      <td>0.875401</td>\n      <td>181.925674</td>\n      <td>12</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>0.01</th>\n      <td>89</td>\n      <td>1</td>\n      <td>0.850160</td>\n      <td>155.191711</td>\n      <td>0.873197</td>\n      <td>150.166595</td>\n      <td>15</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>0.10</th>\n      <td>82</td>\n      <td>1</td>\n      <td>0.855869</td>\n      <td>254.620392</td>\n      <td>0.879607</td>\n      <td>254.343155</td>\n      <td>14</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>1.00</th>\n      <td>96</td>\n      <td>1</td>\n      <td>0.856971</td>\n      <td>251.753006</td>\n      <td>0.879006</td>\n      <td>250.925217</td>\n      <td>13</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">0.00100</th>\n      <th>0.00</th>\n      <td>78</td>\n      <td>0</td>\n      <td>0.827925</td>\n      <td>83.491783</td>\n      <td>0.825254</td>\n      <td>78.086075</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0.01</th>\n      <td>61</td>\n      <td>1</td>\n      <td>0.819111</td>\n      <td>79.707634</td>\n      <td>0.824386</td>\n      <td>81.062035</td>\n      <td>10</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>0.10</th>\n      <td>64</td>\n      <td>1</td>\n      <td>0.827925</td>\n      <td>91.686699</td>\n      <td>0.828926</td>\n      <td>80.465942</td>\n      <td>9</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>1.00</th>\n      <td>98</td>\n      <td>1</td>\n      <td>0.852965</td>\n      <td>250.222351</td>\n      <td>0.862313</td>\n      <td>249.674149</td>\n      <td>8</td>\n      <td>17</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataframe with one per trials\")\n",
    "# mean_df = best_df.groupby(by=[\"cost_per_sample\", \"surprisal_cost\"])\n",
    "sorted = best_df.groupby([\"cost_per_sample\", \"surprisal_cost\"], sort=\"val_acc\")\n",
    "sorted.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearer visualization\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                count  acc_mean   acc_std  updates_mean  \\\ncost_per_sample surprisal_cost                                            \n0.00001         0.00                3  0.852097  0.012045    214.825521   \n                0.01                3  0.850995  0.003505    214.538595   \n                0.10                3  0.856237  0.002570    263.550313   \n                1.00                3  0.855836  0.003788    269.007670   \n0.00005         0.00                3  0.849726  0.004916    193.007614   \n                0.01                3  0.854834  0.005541    188.465342   \n                0.10                2  0.854617  0.002479    263.382202   \n                1.00                3  0.852364  0.006305    263.758779   \n0.00010         0.00                3  0.849259  0.001279    187.587474   \n                0.01                3  0.852264  0.003302    172.887721   \n                0.10                3  0.854100  0.003065    256.334234   \n                1.00                3  0.855202  0.001572    256.080429   \n0.00100         1.00                3  0.850594  0.002515    248.680654   \n\n                                updates_std  epoch_mean  \ncost_per_sample surprisal_cost                           \n0.00001         0.00              24.747241   68.000000  \n                0.01              19.352457   61.333333  \n                0.10               3.553557   86.333333  \n                1.00               5.870964   74.333333  \n0.00005         0.00              19.930493   66.333333  \n                0.01              21.217281   75.000000  \n                0.10               8.445058   68.000000  \n                1.00               3.931146   67.000000  \n0.00010         0.00              12.649807   75.666667  \n                0.01              19.273671   78.000000  \n                0.10               4.471512   84.333333  \n                1.00               4.110700   96.333333  \n0.00100         1.00               3.611668   87.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>count</th>\n      <th>acc_mean</th>\n      <th>acc_std</th>\n      <th>updates_mean</th>\n      <th>updates_std</th>\n      <th>epoch_mean</th>\n    </tr>\n    <tr>\n      <th>cost_per_sample</th>\n      <th>surprisal_cost</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">0.00001</th>\n      <th>0.00</th>\n      <td>3</td>\n      <td>0.852097</td>\n      <td>0.012045</td>\n      <td>214.825521</td>\n      <td>24.747241</td>\n      <td>68.000000</td>\n    </tr>\n    <tr>\n      <th>0.01</th>\n      <td>3</td>\n      <td>0.850995</td>\n      <td>0.003505</td>\n      <td>214.538595</td>\n      <td>19.352457</td>\n      <td>61.333333</td>\n    </tr>\n    <tr>\n      <th>0.10</th>\n      <td>3</td>\n      <td>0.856237</td>\n      <td>0.002570</td>\n      <td>263.550313</td>\n      <td>3.553557</td>\n      <td>86.333333</td>\n    </tr>\n    <tr>\n      <th>1.00</th>\n      <td>3</td>\n      <td>0.855836</td>\n      <td>0.003788</td>\n      <td>269.007670</td>\n      <td>5.870964</td>\n      <td>74.333333</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">0.00005</th>\n      <th>0.00</th>\n      <td>3</td>\n      <td>0.849726</td>\n      <td>0.004916</td>\n      <td>193.007614</td>\n      <td>19.930493</td>\n      <td>66.333333</td>\n    </tr>\n    <tr>\n      <th>0.01</th>\n      <td>3</td>\n      <td>0.854834</td>\n      <td>0.005541</td>\n      <td>188.465342</td>\n      <td>21.217281</td>\n      <td>75.000000</td>\n    </tr>\n    <tr>\n      <th>0.10</th>\n      <td>2</td>\n      <td>0.854617</td>\n      <td>0.002479</td>\n      <td>263.382202</td>\n      <td>8.445058</td>\n      <td>68.000000</td>\n    </tr>\n    <tr>\n      <th>1.00</th>\n      <td>3</td>\n      <td>0.852364</td>\n      <td>0.006305</td>\n      <td>263.758779</td>\n      <td>3.931146</td>\n      <td>67.000000</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">0.00010</th>\n      <th>0.00</th>\n      <td>3</td>\n      <td>0.849259</td>\n      <td>0.001279</td>\n      <td>187.587474</td>\n      <td>12.649807</td>\n      <td>75.666667</td>\n    </tr>\n    <tr>\n      <th>0.01</th>\n      <td>3</td>\n      <td>0.852264</td>\n      <td>0.003302</td>\n      <td>172.887721</td>\n      <td>19.273671</td>\n      <td>78.000000</td>\n    </tr>\n    <tr>\n      <th>0.10</th>\n      <td>3</td>\n      <td>0.854100</td>\n      <td>0.003065</td>\n      <td>256.334234</td>\n      <td>4.471512</td>\n      <td>84.333333</td>\n    </tr>\n    <tr>\n      <th>1.00</th>\n      <td>3</td>\n      <td>0.855202</td>\n      <td>0.001572</td>\n      <td>256.080429</td>\n      <td>4.110700</td>\n      <td>96.333333</td>\n    </tr>\n    <tr>\n      <th>0.00100</th>\n      <th>1.00</th>\n      <td>3</td>\n      <td>0.850594</td>\n      <td>0.002515</td>\n      <td>248.680654</td>\n      <td>3.611668</td>\n      <td>87.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean_df = mean_df.max()\n",
    "# mean_df.sort_values(by='val_acc')\n",
    "print(\"Clearer visualization\")\n",
    "# , axis=1, names=[\"acc_mean\", \"acc_std\", \"updates_mean\", \"updates_std\"])\n",
    "view = pd.DataFrame({'count': sorted.val_acc.count(),\n",
    "                     'acc_mean': sorted.val_acc.mean(),\n",
    "                     'acc_std': sorted.val_acc.std(),\n",
    "                     'updates_mean': sorted.val_updates.mean(),\n",
    "                     'updates_std': sorted.val_updates.std(),\n",
    "                     'epoch_mean': sorted.epoch.mean()})\n",
    "# view.rename(columns=[\"acc_mean\", \"acc_std\", \"updates_mean\", \"updates_std\"])\n",
    "view[view['acc_mean'] > 0.835]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "lr0001 = [csv for csv in csvs if csv['learning_rate'][0]==0.0001 and csv['hidden_units'][0]==32]\n",
    "\n",
    "# temp = bs64_best.loc[(best_df['hidden_units']==96)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:320: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 2160x1440 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not os.path.exists(f\"{folder}/plots\"):\n",
    "    os.makedirs(f\"{folder}/plots\")\n",
    "\n",
    "for i, df in enumerate(csvs):\n",
    "    df.loc[:, ['val_updates', 'train_updates']] = df[['val_updates', 'train_updates']]\n",
    "    ax = df[['val_acc', 'train_acc', 'val_updates', 'train_updates']].plot(figsize= (30, 20))\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.get_figure().savefig(f\"{folder}/plots/idx{i}_acc{round(best_accs[i], 2)}_cps{csvs[i].cost_per_sample[0]}_s{csvs[i].surprisal_cost[0]}_exp{csvs[i].exp[0]}.png\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# if not os.path.exists(f\"{folder}/lrplots\"):\n",
    "#     os.makedirs(f\"{folder}/lrplots\")\n",
    "#\n",
    "# for i, df in enumerate(lr0001):\n",
    "#     df[['val_acc', 'train_acc']].plot().get_figure().savefig(f\"{folder}/lrplots/idx{i}.png\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-36-480a54a8c449>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mbest_hyper\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlr0001\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mbest_hyper_diff\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbest_hyper\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'val_acc'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdiff\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m15\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mbest_hyper_diff\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mabs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001B[0m in \u001B[0;36mconcat\u001B[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001B[0m\n\u001B[0;32m    279\u001B[0m         \u001B[0mverify_integrity\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mverify_integrity\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    280\u001B[0m         \u001B[0mcopy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 281\u001B[1;33m         \u001B[0msort\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msort\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    282\u001B[0m     )\n\u001B[0;32m    283\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001B[0m\n\u001B[0;32m    327\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    328\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobjs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 329\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"No objects to concatenate\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    330\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    331\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mkeys\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "best_hyper = pd.concat(lr0001)\n",
    "\n",
    "best_hyper_diff = best_hyper['val_acc'].diff(15)\n",
    "best_hyper_diff.abs().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csvs[11]\n",
    "\n",
    "best_df.sort_values(by='val_acc')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}