{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%% Import files\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import matplotlib\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "folder = '../../AfterFailure/Gridsearch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Nans: 23\n"
     ]
    }
   ],
   "source": [
    "test_val = 'val'\n",
    "csvs = []\n",
    "\n",
    "count_nan = 0\n",
    "for i, file in enumerate(os.listdir(folder + '/csvs')):\n",
    "    df = pd.read_csv(folder + '/csvs' + '/' + file)\n",
    "    if (df.batch_size == 64).any():\n",
    "        df.rename(columns={'Unnamed: 0' : 'epoch'}, inplace=True)\n",
    "        count_nan += df.shape[0] * df.shape[1] - np.sum(df.count())\n",
    "        df['list_index'] = len(csvs)\n",
    "        filename = file.split(\"_\")\n",
    "        # print(filename[0][-2:].isdigit())\n",
    "        if filename[0][-2:].isdigit(): df['exp'] = filename[0][-2:]\n",
    "        else: df['exp'] = filename[0][-1]\n",
    "        csvs.append(df)\n",
    "\n",
    "print(f\"Total number of Nans: {count_nan}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "nan_index = []\n",
    "for i, df in enumerate(csvs):\n",
    "    nan_index.append(list(df[(df['val_acc'].isnull()) | (df['train_acc'].isnull()) |(df['train_updates'].isnull()) | (df['val_updates'].isnull())].index))\n",
    "\n",
    "print(nan_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# weird_acc = []\n",
    "# for i, df in enumerate(csvs):\n",
    "#     weird_acc.append(list(df[(df['val_acc']>1) | (df['val_acc']<0.1) | (df['train_acc']>1) | (df['train_acc']<0.1)].index))\n",
    "# # for df in csvs:\n",
    "# #     df.drop(df[(df['val_acc']>1) | (df['val_acc']<0)].index, inplace = True)\n",
    "#\n",
    "# see = [csvs[i].iloc[l[0]] for i, l in enumerate(weird_acc) if l]\n",
    "#\n",
    "# early_stopped_dfs = []\n",
    "# for i in range(len(csvs)):\n",
    "#     n = nan_index[i]\n",
    "#     v = weird_acc[i]\n",
    "#     if n and v:\n",
    "#         early_stopped_dfs.append(csvs[i].iloc[[min(min(v), min(n))]])\n",
    "#     elif n:\n",
    "#         early_stopped_dfs.append(csvs[i].iloc[[min(n)]])\n",
    "#     elif v:\n",
    "#         early_stopped_dfs.append(csvs[i].iloc[[min(v)]])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# early_stopped = pd.concat(early_stopped_dfs)\n",
    "# early_stopped.drop(columns=['val_acc', 'train_acc', 'val_updates', 'train_updates', 'early_stopping'], inplace=True)\n",
    "# print(\"Networks that stopped early\")\n",
    "# early_stopped\n",
    "# csvs[0].columns\n",
    "# csvs[2]\n",
    "# for df in csvs:\n",
    "#     print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# manual_early_stopping = {4: 36,5: 28, 12: 12, 14:27, 32:18, 35:23, 41:35, 46: 12,53:12}\n",
    "#\n",
    "# for key, val in manual_early_stopping.items():\n",
    "#     csvs[key].drop(labels = range(val, 40), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             epoch  batch_size  cost_per_sample  hidden_units  learning_rate  \\\ncount  5309.000000      5309.0      5309.000000        5309.0   5.309000e+03   \nmean     42.825391        64.0         0.001366          32.0   2.500000e-04   \nstd      30.783309         0.0         0.003057           0.0   5.421521e-20   \nmin       0.000000        64.0         0.000001          32.0   2.500000e-04   \n25%      17.000000        64.0         0.000010          32.0   2.500000e-04   \n50%      36.000000        64.0         0.000100          32.0   2.500000e-04   \n75%      64.000000        64.0         0.001000          32.0   2.500000e-04   \nmax     124.000000        64.0         0.010000          32.0   2.500000e-04   \n\n       surprisal_cost        trial      val_acc  val_updates    train_acc  \\\ncount     5309.000000  5309.000000  5309.000000  5309.000000  5309.000000   \nmean         0.017200     1.007346     0.703087     0.597269     0.713208   \nstd          0.034958     0.799010     0.117814     0.351186     0.126893   \nmin          0.000000     0.000000     0.492889     0.009201     0.488448   \n25%          0.000100     0.000000     0.585938     0.300430     0.585804   \n50%          0.001000     1.000000     0.711538     0.650675     0.716613   \n75%          0.010000     2.000000     0.811198     0.999107     0.829527   \nmax          0.100000     2.000000     0.858474     1.000000     0.897970   \n\n       train_updates  entropy_loss  budget_loss  surprisal_loss   list_index  \ncount    5309.000000   5286.000000  5309.000000     5309.000000  5309.000000  \nmean        0.596962      0.525008     0.045674        0.056184    35.585986  \nstd         0.348246      0.139807     0.213782        0.143749    21.348068  \nmin         0.009239      0.260116     0.000022        0.000000     0.000000  \n25%         0.305104      0.390306     0.001575        0.000132    17.000000  \n50%         0.636548      0.583787     0.013160        0.001311    36.000000  \n75%         0.999167      0.667520     0.034306        0.039789    54.000000  \nmax         1.000000      0.717025     2.303688        0.687550    73.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>batch_size</th>\n      <th>cost_per_sample</th>\n      <th>hidden_units</th>\n      <th>learning_rate</th>\n      <th>surprisal_cost</th>\n      <th>trial</th>\n      <th>val_acc</th>\n      <th>val_updates</th>\n      <th>train_acc</th>\n      <th>train_updates</th>\n      <th>entropy_loss</th>\n      <th>budget_loss</th>\n      <th>surprisal_loss</th>\n      <th>list_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>5309.000000</td>\n      <td>5309.0</td>\n      <td>5309.000000</td>\n      <td>5309.0</td>\n      <td>5.309000e+03</td>\n      <td>5309.000000</td>\n      <td>5309.000000</td>\n      <td>5309.000000</td>\n      <td>5309.000000</td>\n      <td>5309.000000</td>\n      <td>5309.000000</td>\n      <td>5286.000000</td>\n      <td>5309.000000</td>\n      <td>5309.000000</td>\n      <td>5309.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>42.825391</td>\n      <td>64.0</td>\n      <td>0.001366</td>\n      <td>32.0</td>\n      <td>2.500000e-04</td>\n      <td>0.017200</td>\n      <td>1.007346</td>\n      <td>0.703087</td>\n      <td>0.597269</td>\n      <td>0.713208</td>\n      <td>0.596962</td>\n      <td>0.525008</td>\n      <td>0.045674</td>\n      <td>0.056184</td>\n      <td>35.585986</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>30.783309</td>\n      <td>0.0</td>\n      <td>0.003057</td>\n      <td>0.0</td>\n      <td>5.421521e-20</td>\n      <td>0.034958</td>\n      <td>0.799010</td>\n      <td>0.117814</td>\n      <td>0.351186</td>\n      <td>0.126893</td>\n      <td>0.348246</td>\n      <td>0.139807</td>\n      <td>0.213782</td>\n      <td>0.143749</td>\n      <td>21.348068</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>64.0</td>\n      <td>0.000001</td>\n      <td>32.0</td>\n      <td>2.500000e-04</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.492889</td>\n      <td>0.009201</td>\n      <td>0.488448</td>\n      <td>0.009239</td>\n      <td>0.260116</td>\n      <td>0.000022</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>17.000000</td>\n      <td>64.0</td>\n      <td>0.000010</td>\n      <td>32.0</td>\n      <td>2.500000e-04</td>\n      <td>0.000100</td>\n      <td>0.000000</td>\n      <td>0.585938</td>\n      <td>0.300430</td>\n      <td>0.585804</td>\n      <td>0.305104</td>\n      <td>0.390306</td>\n      <td>0.001575</td>\n      <td>0.000132</td>\n      <td>17.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>36.000000</td>\n      <td>64.0</td>\n      <td>0.000100</td>\n      <td>32.0</td>\n      <td>2.500000e-04</td>\n      <td>0.001000</td>\n      <td>1.000000</td>\n      <td>0.711538</td>\n      <td>0.650675</td>\n      <td>0.716613</td>\n      <td>0.636548</td>\n      <td>0.583787</td>\n      <td>0.013160</td>\n      <td>0.001311</td>\n      <td>36.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>64.000000</td>\n      <td>64.0</td>\n      <td>0.001000</td>\n      <td>32.0</td>\n      <td>2.500000e-04</td>\n      <td>0.010000</td>\n      <td>2.000000</td>\n      <td>0.811198</td>\n      <td>0.999107</td>\n      <td>0.829527</td>\n      <td>0.999167</td>\n      <td>0.667520</td>\n      <td>0.034306</td>\n      <td>0.039789</td>\n      <td>54.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>124.000000</td>\n      <td>64.0</td>\n      <td>0.010000</td>\n      <td>32.0</td>\n      <td>2.500000e-04</td>\n      <td>0.100000</td>\n      <td>2.000000</td>\n      <td>0.858474</td>\n      <td>1.000000</td>\n      <td>0.897970</td>\n      <td>1.000000</td>\n      <td>0.717025</td>\n      <td>2.303688</td>\n      <td>0.687550</td>\n      <td>73.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_df = pd.concat(csvs)\n",
    "big_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x29a92376bc8>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "big_df['val_acc'].hist(figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_rows = []\n",
    "best_accs = []\n",
    "for df in csvs:\n",
    "    best_accs.append(df.loc[:, (test_val + '_acc')].max())\n",
    "    best_rows.append(df.loc[df.loc[:, test_val + '_acc'].argmax()].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_df = pd.DataFrame(best_rows)\n",
    "best_df = best_df.drop(columns= [\"learning_rate\", \"hidden_units\", \"batch_size\", \"early_stopping\"])\n",
    "\n",
    "original = best_df[best_df['surprisal_cost'] == 0]\n",
    "surprisal = best_df[best_df['surprisal_cost'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No surprisal\n"
     ]
    },
    {
     "data": {
      "text/plain": "    epoch  cost_per_sample  surprisal_cost  trial   val_acc  val_updates  \\\n0      99         0.001000             0.0      0  0.825421     0.325568   \n1      99         0.001000             0.0      1  0.833133     0.324010   \n2      50         0.001000             0.0      2  0.804387     0.323899   \n3      22         0.010000             0.0      0  0.588041     0.012794   \n4      24         0.010000             0.0      2  0.582432     0.010479   \n5      49         0.000010             0.0      0  0.845453     0.711011   \n6      94         0.000010             0.0      1  0.855268     0.798062   \n7      98         0.000010             0.0      2  0.851763     0.800462   \n23    123         0.000001             0.0      0  0.855769     0.875970   \n24     34         0.000001             0.0      1  0.709535     0.914011   \n25     21         0.000001             0.0      2  0.664263     0.843289   \n54     62         0.000100             0.0      0  0.846955     0.589769   \n55     93         0.000100             0.0      1  0.853466     0.713798   \n56     76         0.000100             0.0      2  0.846955     0.633213   \n57     41         0.010000             0.0      1  0.592748     0.015043   \n\n    train_acc  train_updates  entropy_loss  budget_loss  surprisal_loss  \\\n0    0.840545       0.310575      0.373079     0.071547             0.0   \n1    0.840612       0.300526      0.370953     0.069232             0.0   \n2    0.807292       0.317303      0.423025     0.073097             0.0   \n3    0.577390       0.012584      0.673652     0.028990             0.0   \n4    0.571381       0.010645      0.675198     0.024523             0.0   \n5    0.844685       0.663304      0.359965     0.001528             0.0   \n6    0.880743       0.775913      0.290202     0.001787             0.0   \n7    0.867054       0.786517      0.326296     0.001812             0.0   \n23   0.889690       0.874434      0.280855     0.000201             0.0   \n24   0.692842       0.920133      0.608738     0.000212             0.0   \n25   0.675548       0.873485      0.621319     0.000201             0.0   \n54   0.850694       0.607837      0.342386     0.014003             0.0   \n55   0.867722       0.648041      0.313555     0.014929             0.0   \n56   0.868456       0.592319      0.315703     0.013645             0.0   \n57   0.585069       0.014018      0.665091     0.032294             0.0   \n\n    list_index exp  \n0            0   0  \n1            1   0  \n2            2   0  \n3            3   0  \n4            4  10  \n5            5  10  \n6            6  10  \n7            7  10  \n23          23  15  \n24          24  15  \n25          25  15  \n54          54   5  \n55          55   5  \n56          56   5  \n57          57   5  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>cost_per_sample</th>\n      <th>surprisal_cost</th>\n      <th>trial</th>\n      <th>val_acc</th>\n      <th>val_updates</th>\n      <th>train_acc</th>\n      <th>train_updates</th>\n      <th>entropy_loss</th>\n      <th>budget_loss</th>\n      <th>surprisal_loss</th>\n      <th>list_index</th>\n      <th>exp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>99</td>\n      <td>0.001000</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.825421</td>\n      <td>0.325568</td>\n      <td>0.840545</td>\n      <td>0.310575</td>\n      <td>0.373079</td>\n      <td>0.071547</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>99</td>\n      <td>0.001000</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0.833133</td>\n      <td>0.324010</td>\n      <td>0.840612</td>\n      <td>0.300526</td>\n      <td>0.370953</td>\n      <td>0.069232</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>50</td>\n      <td>0.001000</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0.804387</td>\n      <td>0.323899</td>\n      <td>0.807292</td>\n      <td>0.317303</td>\n      <td>0.423025</td>\n      <td>0.073097</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>22</td>\n      <td>0.010000</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.588041</td>\n      <td>0.012794</td>\n      <td>0.577390</td>\n      <td>0.012584</td>\n      <td>0.673652</td>\n      <td>0.028990</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24</td>\n      <td>0.010000</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0.582432</td>\n      <td>0.010479</td>\n      <td>0.571381</td>\n      <td>0.010645</td>\n      <td>0.675198</td>\n      <td>0.024523</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>49</td>\n      <td>0.000010</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.845453</td>\n      <td>0.711011</td>\n      <td>0.844685</td>\n      <td>0.663304</td>\n      <td>0.359965</td>\n      <td>0.001528</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>94</td>\n      <td>0.000010</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0.855268</td>\n      <td>0.798062</td>\n      <td>0.880743</td>\n      <td>0.775913</td>\n      <td>0.290202</td>\n      <td>0.001787</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>98</td>\n      <td>0.000010</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0.851763</td>\n      <td>0.800462</td>\n      <td>0.867054</td>\n      <td>0.786517</td>\n      <td>0.326296</td>\n      <td>0.001812</td>\n      <td>0.0</td>\n      <td>7</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>123</td>\n      <td>0.000001</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.855769</td>\n      <td>0.875970</td>\n      <td>0.889690</td>\n      <td>0.874434</td>\n      <td>0.280855</td>\n      <td>0.000201</td>\n      <td>0.0</td>\n      <td>23</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>34</td>\n      <td>0.000001</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0.709535</td>\n      <td>0.914011</td>\n      <td>0.692842</td>\n      <td>0.920133</td>\n      <td>0.608738</td>\n      <td>0.000212</td>\n      <td>0.0</td>\n      <td>24</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>21</td>\n      <td>0.000001</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0.664263</td>\n      <td>0.843289</td>\n      <td>0.675548</td>\n      <td>0.873485</td>\n      <td>0.621319</td>\n      <td>0.000201</td>\n      <td>0.0</td>\n      <td>25</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>62</td>\n      <td>0.000100</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.846955</td>\n      <td>0.589769</td>\n      <td>0.850694</td>\n      <td>0.607837</td>\n      <td>0.342386</td>\n      <td>0.014003</td>\n      <td>0.0</td>\n      <td>54</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>93</td>\n      <td>0.000100</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0.853466</td>\n      <td>0.713798</td>\n      <td>0.867722</td>\n      <td>0.648041</td>\n      <td>0.313555</td>\n      <td>0.014929</td>\n      <td>0.0</td>\n      <td>55</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>76</td>\n      <td>0.000100</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0.846955</td>\n      <td>0.633213</td>\n      <td>0.868456</td>\n      <td>0.592319</td>\n      <td>0.315703</td>\n      <td>0.013645</td>\n      <td>0.0</td>\n      <td>56</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>41</td>\n      <td>0.010000</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0.592748</td>\n      <td>0.015043</td>\n      <td>0.585069</td>\n      <td>0.014018</td>\n      <td>0.665091</td>\n      <td>0.032294</td>\n      <td>0.0</td>\n      <td>57</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"No surprisal\")\n",
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With surprisal\n"
     ]
    },
    {
     "data": {
      "text/plain": "    epoch  cost_per_sample  surprisal_cost  trial   val_acc  val_updates  \\\n8      50         0.010000          0.1000      2  0.613081     0.018552   \n9      10         0.000010          0.1000      0  0.678786     1.000000   \n10     87         0.000010          0.1000      1  0.718049     0.999290   \n11     64         0.000010          0.1000      2  0.694311     0.999987   \n12     22         0.010000          0.0100      2  0.592748     0.013174   \n13     77         0.000010          0.0100      0  0.662159     0.999626   \n14    122         0.000010          0.0100      1  0.720152     0.998130   \n15     21         0.000010          0.0100      2  0.690505     0.999998   \n16     33         0.000010          0.0010      0  0.708333     0.999330   \n17     67         0.000010          0.0010      1  0.721054     0.999322   \n18     92         0.000010          0.0010      2  0.708133     0.998854   \n19     23         0.010000          0.0001      2  0.582933     0.011154   \n20     15         0.000010          0.0001      0  0.661358     0.362574   \n21    114         0.000010          0.0001      1  0.858474     0.712259   \n22    117         0.000010          0.0001      2  0.852163     0.780638   \n26     15         0.000001          0.1000      0  0.513622     0.999894   \n27     12         0.000001          0.1000      1  0.510417     0.999874   \n28     31         0.000001          0.1000      2  0.673878     0.999799   \n29     27         0.000001          0.0100      0  0.706330     0.999887   \n30     70         0.000001          0.0100      1  0.700321     0.999218   \n31     66         0.000001          0.0100      2  0.714343     0.999357   \n32     48         0.000001          0.0010      0  0.723357     0.999952   \n33    101         0.000001          0.0010      1  0.843349     0.756398   \n34     31         0.000001          0.0010      2  0.698718     0.999960   \n35     21         0.000001          0.0001      0  0.699519     0.628990   \n36    123         0.000001          0.0001      1  0.847456     0.911272   \n37     63         0.000001          0.0001      2  0.852664     0.816436   \n38     48         0.001000          0.1000      0  0.815805     0.344390   \n39     27         0.001000          0.1000      1  0.673377     0.999954   \n40     20         0.001000          0.1000      2  0.673778     0.999418   \n41     29         0.010000          0.1000      0  0.506510     0.999645   \n42     85         0.001000          0.0100      0  0.827424     0.353876   \n43     86         0.001000          0.0100      1  0.831931     0.353332   \n44     56         0.001000          0.0100      2  0.811599     0.318962   \n45     43         0.010000          0.0100      0  0.591146     0.013093   \n46    121         0.001000          0.0010      0  0.838141     0.327635   \n47     86         0.001000          0.0010      1  0.828826     0.332170   \n48     55         0.001000          0.0010      2  0.815505     0.336793   \n49     22         0.010000          0.0010      0  0.587841     0.012087   \n50     92         0.001000          0.0001      0  0.820312     0.318044   \n51     76         0.001000          0.0001      1  0.829127     0.325993   \n52     78         0.001000          0.0001      2  0.823317     0.314722   \n53     15         0.010000          0.0001      0  0.581030     0.011501   \n58     28         0.000100          0.1000      0  0.680389     0.999942   \n59     54         0.000100          0.1000      1  0.831030     0.999500   \n60     74         0.000100          0.1000      2  0.697616     0.999693   \n61      6         0.010000          0.1000      1  0.584235     0.013731   \n62     78         0.000100          0.0100      0  0.858474     0.657062   \n63     49         0.000100          0.0100      1  0.710938     0.999270   \n64     18         0.000100          0.0100      2  0.648037     0.172032   \n65     15         0.010000          0.0100      1  0.582332     0.012369   \n66     29         0.000100          0.0010      0  0.680990     0.997124   \n67     33         0.000100          0.0010      1  0.708333     0.999162   \n68    123         0.000100          0.0010      2  0.850060     0.743285   \n69     31         0.010000          0.0010      1  0.591246     0.011782   \n70    123         0.000100          0.0001      0  0.855869     0.680365   \n71     17         0.000100          0.0001      1  0.658053     0.236992   \n72     79         0.000100          0.0001      2  0.854868     0.656580   \n73     15         0.010000          0.0001      1  0.576823     0.009751   \n\n    train_acc  train_updates  entropy_loss  budget_loss  surprisal_loss  \\\n8    0.601429       0.018664      0.656518     0.042996        0.686658   \n9    0.667935       0.999998      0.634864     0.002304        0.016618   \n10   0.705395       0.999511      0.599830     0.002303        0.392359   \n11   0.689837       0.999886      0.612159     0.002303        0.092423   \n12   0.587607       0.012701      0.671316     0.029260        0.068657   \n13   0.646100       0.999258      0.641715     0.002302        0.040500   \n14   0.728098       0.998255      0.578905     0.002300        0.042786   \n15   0.681958       0.999999      0.620628     0.002304        0.000888   \n16   0.690772       0.999352      0.613294     0.002302        0.004043   \n17   0.710670       0.999204      0.594854     0.002302        0.004129   \n18   0.706130       0.998787      0.594479     0.002301        0.005119   \n19   0.576389       0.010682      0.673958     0.024607        0.000687   \n20   0.643096       0.289639      0.647239     0.000667        0.000687   \n21   0.889223       0.678831      0.280515     0.001564        0.000682   \n22   0.884882       0.775863      0.288565     0.001787        0.000685   \n26   0.513822       0.999874      0.688704     0.000230        0.485277   \n27   0.513021       0.999930      0.688635     0.000230        0.353730   \n28   0.668403       0.999790      0.633681     0.000230        0.371828   \n29   0.697316       0.999951      0.610662     0.000230        0.019398   \n30   0.682091       0.999241      0.620210     0.000230        0.037630   \n31   0.711538       0.999430      0.598560     0.000230        0.032759   \n32   0.716747       0.999962      0.589404     0.000230        0.001981   \n33   0.864583       0.727726      0.360084     0.000168        0.006870   \n34   0.684028       0.999959      0.620913     0.000230        0.002779   \n35   0.682692       0.726889      0.618008     0.000167        0.000670   \n36   0.879407       0.870021      0.314298     0.000200        0.000686   \n37   0.859241       0.825813      0.333312     0.000190        0.000697   \n38   0.810096       0.314335      0.417116     0.072413        0.684428   \n39   0.664463       0.999479      0.638601     0.230250        0.253199   \n40   0.664530       0.999916      0.636984     0.230351        0.303331   \n41   0.512420       0.999517      0.690491     2.302586        0.306928   \n42   0.834535       0.312422      0.381554     0.071973        0.068578   \n43   0.835737       0.318215      0.372856     0.073307        0.068544   \n44   0.810964       0.308029      0.417550     0.070961        0.068675   \n45   0.582732       0.012734      0.667858     0.029334        0.068657   \n46   0.842214       0.291738      0.369274     0.067208        0.006859   \n47   0.832599       0.294513      0.387923     0.067847        0.006857   \n48   0.818510       0.285521      0.411667     0.065775        0.006862   \n49   0.581330       0.011355      0.674210     0.026159        0.006866   \n50   0.835270       0.305310      0.378612     0.070334        0.000686   \n51   0.835737       0.304190      0.379761     0.070076        0.000687   \n52   0.831464       0.314229      0.388586     0.072389        0.000686   \n53   0.577324       0.010984      0.676664     0.025304        0.000687   \n58   0.635016       0.999879      0.636884     0.023034        0.345492   \n59   0.825321       0.999674      0.394328     0.023029        0.450966   \n60   0.685029       0.999359      0.618443     0.023022        0.412860   \n61   0.572249       0.013273      0.676222     0.030577        0.686543   \n62   0.870192       0.612886      0.309114     0.014119        0.068619   \n63   0.704394       0.999391      0.603715     0.023023        0.037278   \n64   0.639423       0.167365      0.647559     0.003856        0.068535   \n65   0.575855       0.011990      0.675256     0.027622        0.068653   \n66   0.671207       0.997477      0.629237     0.022979        0.006047   \n67   0.700321       0.999755      0.599425     0.023031        0.004677   \n68   0.890091       0.697158      0.273068     0.016060        0.006856   \n69   0.580128       0.011641      0.672541     0.026818        0.006865   \n70   0.884014       0.630347      0.289238     0.014521        0.000687   \n71   0.599426       0.208962      0.666853     0.004814        0.000685   \n72   0.865518       0.584586      0.321883     0.013467        0.000688   \n73   0.569712       0.009724      0.677849     0.022402        0.000686   \n\n    list_index exp  \n8            8  11  \n9            9  11  \n10          10  11  \n11          11  11  \n12          12  12  \n13          13  12  \n14          14  12  \n15          15  12  \n16          16  13  \n17          17  13  \n18          18  13  \n19          19  14  \n20          20  14  \n21          21  14  \n22          22  14  \n26          26  16  \n27          27  16  \n28          28  16  \n29          29  17  \n30          30  17  \n31          31  17  \n32          32  18  \n33          33  18  \n34          34  18  \n35          35  19  \n36          36  19  \n37          37  19  \n38          38   1  \n39          39   1  \n40          40   1  \n41          41   1  \n42          42   2  \n43          43   2  \n44          44   2  \n45          45   2  \n46          46   3  \n47          47   3  \n48          48   3  \n49          49   3  \n50          50   4  \n51          51   4  \n52          52   4  \n53          53   4  \n58          58   6  \n59          59   6  \n60          60   6  \n61          61   6  \n62          62   7  \n63          63   7  \n64          64   7  \n65          65   7  \n66          66   8  \n67          67   8  \n68          68   8  \n69          69   8  \n70          70   9  \n71          71   9  \n72          72   9  \n73          73   9  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>cost_per_sample</th>\n      <th>surprisal_cost</th>\n      <th>trial</th>\n      <th>val_acc</th>\n      <th>val_updates</th>\n      <th>train_acc</th>\n      <th>train_updates</th>\n      <th>entropy_loss</th>\n      <th>budget_loss</th>\n      <th>surprisal_loss</th>\n      <th>list_index</th>\n      <th>exp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>50</td>\n      <td>0.010000</td>\n      <td>0.1000</td>\n      <td>2</td>\n      <td>0.613081</td>\n      <td>0.018552</td>\n      <td>0.601429</td>\n      <td>0.018664</td>\n      <td>0.656518</td>\n      <td>0.042996</td>\n      <td>0.686658</td>\n      <td>8</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>0.000010</td>\n      <td>0.1000</td>\n      <td>0</td>\n      <td>0.678786</td>\n      <td>1.000000</td>\n      <td>0.667935</td>\n      <td>0.999998</td>\n      <td>0.634864</td>\n      <td>0.002304</td>\n      <td>0.016618</td>\n      <td>9</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>87</td>\n      <td>0.000010</td>\n      <td>0.1000</td>\n      <td>1</td>\n      <td>0.718049</td>\n      <td>0.999290</td>\n      <td>0.705395</td>\n      <td>0.999511</td>\n      <td>0.599830</td>\n      <td>0.002303</td>\n      <td>0.392359</td>\n      <td>10</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>64</td>\n      <td>0.000010</td>\n      <td>0.1000</td>\n      <td>2</td>\n      <td>0.694311</td>\n      <td>0.999987</td>\n      <td>0.689837</td>\n      <td>0.999886</td>\n      <td>0.612159</td>\n      <td>0.002303</td>\n      <td>0.092423</td>\n      <td>11</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>22</td>\n      <td>0.010000</td>\n      <td>0.0100</td>\n      <td>2</td>\n      <td>0.592748</td>\n      <td>0.013174</td>\n      <td>0.587607</td>\n      <td>0.012701</td>\n      <td>0.671316</td>\n      <td>0.029260</td>\n      <td>0.068657</td>\n      <td>12</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>77</td>\n      <td>0.000010</td>\n      <td>0.0100</td>\n      <td>0</td>\n      <td>0.662159</td>\n      <td>0.999626</td>\n      <td>0.646100</td>\n      <td>0.999258</td>\n      <td>0.641715</td>\n      <td>0.002302</td>\n      <td>0.040500</td>\n      <td>13</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>122</td>\n      <td>0.000010</td>\n      <td>0.0100</td>\n      <td>1</td>\n      <td>0.720152</td>\n      <td>0.998130</td>\n      <td>0.728098</td>\n      <td>0.998255</td>\n      <td>0.578905</td>\n      <td>0.002300</td>\n      <td>0.042786</td>\n      <td>14</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>21</td>\n      <td>0.000010</td>\n      <td>0.0100</td>\n      <td>2</td>\n      <td>0.690505</td>\n      <td>0.999998</td>\n      <td>0.681958</td>\n      <td>0.999999</td>\n      <td>0.620628</td>\n      <td>0.002304</td>\n      <td>0.000888</td>\n      <td>15</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>33</td>\n      <td>0.000010</td>\n      <td>0.0010</td>\n      <td>0</td>\n      <td>0.708333</td>\n      <td>0.999330</td>\n      <td>0.690772</td>\n      <td>0.999352</td>\n      <td>0.613294</td>\n      <td>0.002302</td>\n      <td>0.004043</td>\n      <td>16</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>67</td>\n      <td>0.000010</td>\n      <td>0.0010</td>\n      <td>1</td>\n      <td>0.721054</td>\n      <td>0.999322</td>\n      <td>0.710670</td>\n      <td>0.999204</td>\n      <td>0.594854</td>\n      <td>0.002302</td>\n      <td>0.004129</td>\n      <td>17</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>92</td>\n      <td>0.000010</td>\n      <td>0.0010</td>\n      <td>2</td>\n      <td>0.708133</td>\n      <td>0.998854</td>\n      <td>0.706130</td>\n      <td>0.998787</td>\n      <td>0.594479</td>\n      <td>0.002301</td>\n      <td>0.005119</td>\n      <td>18</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>23</td>\n      <td>0.010000</td>\n      <td>0.0001</td>\n      <td>2</td>\n      <td>0.582933</td>\n      <td>0.011154</td>\n      <td>0.576389</td>\n      <td>0.010682</td>\n      <td>0.673958</td>\n      <td>0.024607</td>\n      <td>0.000687</td>\n      <td>19</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>15</td>\n      <td>0.000010</td>\n      <td>0.0001</td>\n      <td>0</td>\n      <td>0.661358</td>\n      <td>0.362574</td>\n      <td>0.643096</td>\n      <td>0.289639</td>\n      <td>0.647239</td>\n      <td>0.000667</td>\n      <td>0.000687</td>\n      <td>20</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>114</td>\n      <td>0.000010</td>\n      <td>0.0001</td>\n      <td>1</td>\n      <td>0.858474</td>\n      <td>0.712259</td>\n      <td>0.889223</td>\n      <td>0.678831</td>\n      <td>0.280515</td>\n      <td>0.001564</td>\n      <td>0.000682</td>\n      <td>21</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>117</td>\n      <td>0.000010</td>\n      <td>0.0001</td>\n      <td>2</td>\n      <td>0.852163</td>\n      <td>0.780638</td>\n      <td>0.884882</td>\n      <td>0.775863</td>\n      <td>0.288565</td>\n      <td>0.001787</td>\n      <td>0.000685</td>\n      <td>22</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>15</td>\n      <td>0.000001</td>\n      <td>0.1000</td>\n      <td>0</td>\n      <td>0.513622</td>\n      <td>0.999894</td>\n      <td>0.513822</td>\n      <td>0.999874</td>\n      <td>0.688704</td>\n      <td>0.000230</td>\n      <td>0.485277</td>\n      <td>26</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>12</td>\n      <td>0.000001</td>\n      <td>0.1000</td>\n      <td>1</td>\n      <td>0.510417</td>\n      <td>0.999874</td>\n      <td>0.513021</td>\n      <td>0.999930</td>\n      <td>0.688635</td>\n      <td>0.000230</td>\n      <td>0.353730</td>\n      <td>27</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>31</td>\n      <td>0.000001</td>\n      <td>0.1000</td>\n      <td>2</td>\n      <td>0.673878</td>\n      <td>0.999799</td>\n      <td>0.668403</td>\n      <td>0.999790</td>\n      <td>0.633681</td>\n      <td>0.000230</td>\n      <td>0.371828</td>\n      <td>28</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>27</td>\n      <td>0.000001</td>\n      <td>0.0100</td>\n      <td>0</td>\n      <td>0.706330</td>\n      <td>0.999887</td>\n      <td>0.697316</td>\n      <td>0.999951</td>\n      <td>0.610662</td>\n      <td>0.000230</td>\n      <td>0.019398</td>\n      <td>29</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>70</td>\n      <td>0.000001</td>\n      <td>0.0100</td>\n      <td>1</td>\n      <td>0.700321</td>\n      <td>0.999218</td>\n      <td>0.682091</td>\n      <td>0.999241</td>\n      <td>0.620210</td>\n      <td>0.000230</td>\n      <td>0.037630</td>\n      <td>30</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>66</td>\n      <td>0.000001</td>\n      <td>0.0100</td>\n      <td>2</td>\n      <td>0.714343</td>\n      <td>0.999357</td>\n      <td>0.711538</td>\n      <td>0.999430</td>\n      <td>0.598560</td>\n      <td>0.000230</td>\n      <td>0.032759</td>\n      <td>31</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>48</td>\n      <td>0.000001</td>\n      <td>0.0010</td>\n      <td>0</td>\n      <td>0.723357</td>\n      <td>0.999952</td>\n      <td>0.716747</td>\n      <td>0.999962</td>\n      <td>0.589404</td>\n      <td>0.000230</td>\n      <td>0.001981</td>\n      <td>32</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>101</td>\n      <td>0.000001</td>\n      <td>0.0010</td>\n      <td>1</td>\n      <td>0.843349</td>\n      <td>0.756398</td>\n      <td>0.864583</td>\n      <td>0.727726</td>\n      <td>0.360084</td>\n      <td>0.000168</td>\n      <td>0.006870</td>\n      <td>33</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>31</td>\n      <td>0.000001</td>\n      <td>0.0010</td>\n      <td>2</td>\n      <td>0.698718</td>\n      <td>0.999960</td>\n      <td>0.684028</td>\n      <td>0.999959</td>\n      <td>0.620913</td>\n      <td>0.000230</td>\n      <td>0.002779</td>\n      <td>34</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>21</td>\n      <td>0.000001</td>\n      <td>0.0001</td>\n      <td>0</td>\n      <td>0.699519</td>\n      <td>0.628990</td>\n      <td>0.682692</td>\n      <td>0.726889</td>\n      <td>0.618008</td>\n      <td>0.000167</td>\n      <td>0.000670</td>\n      <td>35</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>123</td>\n      <td>0.000001</td>\n      <td>0.0001</td>\n      <td>1</td>\n      <td>0.847456</td>\n      <td>0.911272</td>\n      <td>0.879407</td>\n      <td>0.870021</td>\n      <td>0.314298</td>\n      <td>0.000200</td>\n      <td>0.000686</td>\n      <td>36</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>63</td>\n      <td>0.000001</td>\n      <td>0.0001</td>\n      <td>2</td>\n      <td>0.852664</td>\n      <td>0.816436</td>\n      <td>0.859241</td>\n      <td>0.825813</td>\n      <td>0.333312</td>\n      <td>0.000190</td>\n      <td>0.000697</td>\n      <td>37</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>48</td>\n      <td>0.001000</td>\n      <td>0.1000</td>\n      <td>0</td>\n      <td>0.815805</td>\n      <td>0.344390</td>\n      <td>0.810096</td>\n      <td>0.314335</td>\n      <td>0.417116</td>\n      <td>0.072413</td>\n      <td>0.684428</td>\n      <td>38</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>27</td>\n      <td>0.001000</td>\n      <td>0.1000</td>\n      <td>1</td>\n      <td>0.673377</td>\n      <td>0.999954</td>\n      <td>0.664463</td>\n      <td>0.999479</td>\n      <td>0.638601</td>\n      <td>0.230250</td>\n      <td>0.253199</td>\n      <td>39</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>20</td>\n      <td>0.001000</td>\n      <td>0.1000</td>\n      <td>2</td>\n      <td>0.673778</td>\n      <td>0.999418</td>\n      <td>0.664530</td>\n      <td>0.999916</td>\n      <td>0.636984</td>\n      <td>0.230351</td>\n      <td>0.303331</td>\n      <td>40</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>29</td>\n      <td>0.010000</td>\n      <td>0.1000</td>\n      <td>0</td>\n      <td>0.506510</td>\n      <td>0.999645</td>\n      <td>0.512420</td>\n      <td>0.999517</td>\n      <td>0.690491</td>\n      <td>2.302586</td>\n      <td>0.306928</td>\n      <td>41</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>85</td>\n      <td>0.001000</td>\n      <td>0.0100</td>\n      <td>0</td>\n      <td>0.827424</td>\n      <td>0.353876</td>\n      <td>0.834535</td>\n      <td>0.312422</td>\n      <td>0.381554</td>\n      <td>0.071973</td>\n      <td>0.068578</td>\n      <td>42</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>86</td>\n      <td>0.001000</td>\n      <td>0.0100</td>\n      <td>1</td>\n      <td>0.831931</td>\n      <td>0.353332</td>\n      <td>0.835737</td>\n      <td>0.318215</td>\n      <td>0.372856</td>\n      <td>0.073307</td>\n      <td>0.068544</td>\n      <td>43</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>56</td>\n      <td>0.001000</td>\n      <td>0.0100</td>\n      <td>2</td>\n      <td>0.811599</td>\n      <td>0.318962</td>\n      <td>0.810964</td>\n      <td>0.308029</td>\n      <td>0.417550</td>\n      <td>0.070961</td>\n      <td>0.068675</td>\n      <td>44</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>43</td>\n      <td>0.010000</td>\n      <td>0.0100</td>\n      <td>0</td>\n      <td>0.591146</td>\n      <td>0.013093</td>\n      <td>0.582732</td>\n      <td>0.012734</td>\n      <td>0.667858</td>\n      <td>0.029334</td>\n      <td>0.068657</td>\n      <td>45</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>121</td>\n      <td>0.001000</td>\n      <td>0.0010</td>\n      <td>0</td>\n      <td>0.838141</td>\n      <td>0.327635</td>\n      <td>0.842214</td>\n      <td>0.291738</td>\n      <td>0.369274</td>\n      <td>0.067208</td>\n      <td>0.006859</td>\n      <td>46</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>86</td>\n      <td>0.001000</td>\n      <td>0.0010</td>\n      <td>1</td>\n      <td>0.828826</td>\n      <td>0.332170</td>\n      <td>0.832599</td>\n      <td>0.294513</td>\n      <td>0.387923</td>\n      <td>0.067847</td>\n      <td>0.006857</td>\n      <td>47</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>55</td>\n      <td>0.001000</td>\n      <td>0.0010</td>\n      <td>2</td>\n      <td>0.815505</td>\n      <td>0.336793</td>\n      <td>0.818510</td>\n      <td>0.285521</td>\n      <td>0.411667</td>\n      <td>0.065775</td>\n      <td>0.006862</td>\n      <td>48</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>22</td>\n      <td>0.010000</td>\n      <td>0.0010</td>\n      <td>0</td>\n      <td>0.587841</td>\n      <td>0.012087</td>\n      <td>0.581330</td>\n      <td>0.011355</td>\n      <td>0.674210</td>\n      <td>0.026159</td>\n      <td>0.006866</td>\n      <td>49</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>92</td>\n      <td>0.001000</td>\n      <td>0.0001</td>\n      <td>0</td>\n      <td>0.820312</td>\n      <td>0.318044</td>\n      <td>0.835270</td>\n      <td>0.305310</td>\n      <td>0.378612</td>\n      <td>0.070334</td>\n      <td>0.000686</td>\n      <td>50</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>76</td>\n      <td>0.001000</td>\n      <td>0.0001</td>\n      <td>1</td>\n      <td>0.829127</td>\n      <td>0.325993</td>\n      <td>0.835737</td>\n      <td>0.304190</td>\n      <td>0.379761</td>\n      <td>0.070076</td>\n      <td>0.000687</td>\n      <td>51</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>78</td>\n      <td>0.001000</td>\n      <td>0.0001</td>\n      <td>2</td>\n      <td>0.823317</td>\n      <td>0.314722</td>\n      <td>0.831464</td>\n      <td>0.314229</td>\n      <td>0.388586</td>\n      <td>0.072389</td>\n      <td>0.000686</td>\n      <td>52</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>15</td>\n      <td>0.010000</td>\n      <td>0.0001</td>\n      <td>0</td>\n      <td>0.581030</td>\n      <td>0.011501</td>\n      <td>0.577324</td>\n      <td>0.010984</td>\n      <td>0.676664</td>\n      <td>0.025304</td>\n      <td>0.000687</td>\n      <td>53</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>28</td>\n      <td>0.000100</td>\n      <td>0.1000</td>\n      <td>0</td>\n      <td>0.680389</td>\n      <td>0.999942</td>\n      <td>0.635016</td>\n      <td>0.999879</td>\n      <td>0.636884</td>\n      <td>0.023034</td>\n      <td>0.345492</td>\n      <td>58</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>54</td>\n      <td>0.000100</td>\n      <td>0.1000</td>\n      <td>1</td>\n      <td>0.831030</td>\n      <td>0.999500</td>\n      <td>0.825321</td>\n      <td>0.999674</td>\n      <td>0.394328</td>\n      <td>0.023029</td>\n      <td>0.450966</td>\n      <td>59</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>74</td>\n      <td>0.000100</td>\n      <td>0.1000</td>\n      <td>2</td>\n      <td>0.697616</td>\n      <td>0.999693</td>\n      <td>0.685029</td>\n      <td>0.999359</td>\n      <td>0.618443</td>\n      <td>0.023022</td>\n      <td>0.412860</td>\n      <td>60</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>6</td>\n      <td>0.010000</td>\n      <td>0.1000</td>\n      <td>1</td>\n      <td>0.584235</td>\n      <td>0.013731</td>\n      <td>0.572249</td>\n      <td>0.013273</td>\n      <td>0.676222</td>\n      <td>0.030577</td>\n      <td>0.686543</td>\n      <td>61</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>78</td>\n      <td>0.000100</td>\n      <td>0.0100</td>\n      <td>0</td>\n      <td>0.858474</td>\n      <td>0.657062</td>\n      <td>0.870192</td>\n      <td>0.612886</td>\n      <td>0.309114</td>\n      <td>0.014119</td>\n      <td>0.068619</td>\n      <td>62</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>49</td>\n      <td>0.000100</td>\n      <td>0.0100</td>\n      <td>1</td>\n      <td>0.710938</td>\n      <td>0.999270</td>\n      <td>0.704394</td>\n      <td>0.999391</td>\n      <td>0.603715</td>\n      <td>0.023023</td>\n      <td>0.037278</td>\n      <td>63</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>18</td>\n      <td>0.000100</td>\n      <td>0.0100</td>\n      <td>2</td>\n      <td>0.648037</td>\n      <td>0.172032</td>\n      <td>0.639423</td>\n      <td>0.167365</td>\n      <td>0.647559</td>\n      <td>0.003856</td>\n      <td>0.068535</td>\n      <td>64</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>15</td>\n      <td>0.010000</td>\n      <td>0.0100</td>\n      <td>1</td>\n      <td>0.582332</td>\n      <td>0.012369</td>\n      <td>0.575855</td>\n      <td>0.011990</td>\n      <td>0.675256</td>\n      <td>0.027622</td>\n      <td>0.068653</td>\n      <td>65</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>29</td>\n      <td>0.000100</td>\n      <td>0.0010</td>\n      <td>0</td>\n      <td>0.680990</td>\n      <td>0.997124</td>\n      <td>0.671207</td>\n      <td>0.997477</td>\n      <td>0.629237</td>\n      <td>0.022979</td>\n      <td>0.006047</td>\n      <td>66</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>33</td>\n      <td>0.000100</td>\n      <td>0.0010</td>\n      <td>1</td>\n      <td>0.708333</td>\n      <td>0.999162</td>\n      <td>0.700321</td>\n      <td>0.999755</td>\n      <td>0.599425</td>\n      <td>0.023031</td>\n      <td>0.004677</td>\n      <td>67</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>123</td>\n      <td>0.000100</td>\n      <td>0.0010</td>\n      <td>2</td>\n      <td>0.850060</td>\n      <td>0.743285</td>\n      <td>0.890091</td>\n      <td>0.697158</td>\n      <td>0.273068</td>\n      <td>0.016060</td>\n      <td>0.006856</td>\n      <td>68</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>31</td>\n      <td>0.010000</td>\n      <td>0.0010</td>\n      <td>1</td>\n      <td>0.591246</td>\n      <td>0.011782</td>\n      <td>0.580128</td>\n      <td>0.011641</td>\n      <td>0.672541</td>\n      <td>0.026818</td>\n      <td>0.006865</td>\n      <td>69</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>123</td>\n      <td>0.000100</td>\n      <td>0.0001</td>\n      <td>0</td>\n      <td>0.855869</td>\n      <td>0.680365</td>\n      <td>0.884014</td>\n      <td>0.630347</td>\n      <td>0.289238</td>\n      <td>0.014521</td>\n      <td>0.000687</td>\n      <td>70</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>17</td>\n      <td>0.000100</td>\n      <td>0.0001</td>\n      <td>1</td>\n      <td>0.658053</td>\n      <td>0.236992</td>\n      <td>0.599426</td>\n      <td>0.208962</td>\n      <td>0.666853</td>\n      <td>0.004814</td>\n      <td>0.000685</td>\n      <td>71</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>79</td>\n      <td>0.000100</td>\n      <td>0.0001</td>\n      <td>2</td>\n      <td>0.854868</td>\n      <td>0.656580</td>\n      <td>0.865518</td>\n      <td>0.584586</td>\n      <td>0.321883</td>\n      <td>0.013467</td>\n      <td>0.000688</td>\n      <td>72</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>15</td>\n      <td>0.010000</td>\n      <td>0.0001</td>\n      <td>1</td>\n      <td>0.576823</td>\n      <td>0.009751</td>\n      <td>0.569712</td>\n      <td>0.009724</td>\n      <td>0.677849</td>\n      <td>0.022402</td>\n      <td>0.000686</td>\n      <td>73</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"With surprisal\")\n",
    "surprisal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with all best epochs for Validation accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": "    epoch  cost_per_sample  surprisal_cost  trial   val_acc  val_updates  \\\n61      6         0.010000          0.1000      1  0.584235     0.013731   \n9      10         0.000010          0.1000      0  0.678786     1.000000   \n27     12         0.000001          0.1000      1  0.510417     0.999874   \n65     15         0.010000          0.0100      1  0.582332     0.012369   \n20     15         0.000010          0.0001      0  0.661358     0.362574   \n..    ...              ...             ...    ...       ...          ...   \n14    122         0.000010          0.0100      1  0.720152     0.998130   \n23    123         0.000001          0.0000      0  0.855769     0.875970   \n68    123         0.000100          0.0010      2  0.850060     0.743285   \n70    123         0.000100          0.0001      0  0.855869     0.680365   \n36    123         0.000001          0.0001      1  0.847456     0.911272   \n\n    train_acc  train_updates  entropy_loss  budget_loss  surprisal_loss  \\\n61   0.572249       0.013273      0.676222     0.030577        0.686543   \n9    0.667935       0.999998      0.634864     0.002304        0.016618   \n27   0.513021       0.999930      0.688635     0.000230        0.353730   \n65   0.575855       0.011990      0.675256     0.027622        0.068653   \n20   0.643096       0.289639      0.647239     0.000667        0.000687   \n..        ...            ...           ...          ...             ...   \n14   0.728098       0.998255      0.578905     0.002300        0.042786   \n23   0.889690       0.874434      0.280855     0.000201        0.000000   \n68   0.890091       0.697158      0.273068     0.016060        0.006856   \n70   0.884014       0.630347      0.289238     0.014521        0.000687   \n36   0.879407       0.870021      0.314298     0.000200        0.000686   \n\n    list_index exp  \n61          61   6  \n9            9  11  \n27          27  16  \n65          65   7  \n20          20  14  \n..         ...  ..  \n14          14  12  \n23          23  15  \n68          68   8  \n70          70   9  \n36          36  19  \n\n[74 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>cost_per_sample</th>\n      <th>surprisal_cost</th>\n      <th>trial</th>\n      <th>val_acc</th>\n      <th>val_updates</th>\n      <th>train_acc</th>\n      <th>train_updates</th>\n      <th>entropy_loss</th>\n      <th>budget_loss</th>\n      <th>surprisal_loss</th>\n      <th>list_index</th>\n      <th>exp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>61</th>\n      <td>6</td>\n      <td>0.010000</td>\n      <td>0.1000</td>\n      <td>1</td>\n      <td>0.584235</td>\n      <td>0.013731</td>\n      <td>0.572249</td>\n      <td>0.013273</td>\n      <td>0.676222</td>\n      <td>0.030577</td>\n      <td>0.686543</td>\n      <td>61</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>0.000010</td>\n      <td>0.1000</td>\n      <td>0</td>\n      <td>0.678786</td>\n      <td>1.000000</td>\n      <td>0.667935</td>\n      <td>0.999998</td>\n      <td>0.634864</td>\n      <td>0.002304</td>\n      <td>0.016618</td>\n      <td>9</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>12</td>\n      <td>0.000001</td>\n      <td>0.1000</td>\n      <td>1</td>\n      <td>0.510417</td>\n      <td>0.999874</td>\n      <td>0.513021</td>\n      <td>0.999930</td>\n      <td>0.688635</td>\n      <td>0.000230</td>\n      <td>0.353730</td>\n      <td>27</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>15</td>\n      <td>0.010000</td>\n      <td>0.0100</td>\n      <td>1</td>\n      <td>0.582332</td>\n      <td>0.012369</td>\n      <td>0.575855</td>\n      <td>0.011990</td>\n      <td>0.675256</td>\n      <td>0.027622</td>\n      <td>0.068653</td>\n      <td>65</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>15</td>\n      <td>0.000010</td>\n      <td>0.0001</td>\n      <td>0</td>\n      <td>0.661358</td>\n      <td>0.362574</td>\n      <td>0.643096</td>\n      <td>0.289639</td>\n      <td>0.647239</td>\n      <td>0.000667</td>\n      <td>0.000687</td>\n      <td>20</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>122</td>\n      <td>0.000010</td>\n      <td>0.0100</td>\n      <td>1</td>\n      <td>0.720152</td>\n      <td>0.998130</td>\n      <td>0.728098</td>\n      <td>0.998255</td>\n      <td>0.578905</td>\n      <td>0.002300</td>\n      <td>0.042786</td>\n      <td>14</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>123</td>\n      <td>0.000001</td>\n      <td>0.0000</td>\n      <td>0</td>\n      <td>0.855769</td>\n      <td>0.875970</td>\n      <td>0.889690</td>\n      <td>0.874434</td>\n      <td>0.280855</td>\n      <td>0.000201</td>\n      <td>0.000000</td>\n      <td>23</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>123</td>\n      <td>0.000100</td>\n      <td>0.0010</td>\n      <td>2</td>\n      <td>0.850060</td>\n      <td>0.743285</td>\n      <td>0.890091</td>\n      <td>0.697158</td>\n      <td>0.273068</td>\n      <td>0.016060</td>\n      <td>0.006856</td>\n      <td>68</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>123</td>\n      <td>0.000100</td>\n      <td>0.0001</td>\n      <td>0</td>\n      <td>0.855869</td>\n      <td>0.680365</td>\n      <td>0.884014</td>\n      <td>0.630347</td>\n      <td>0.289238</td>\n      <td>0.014521</td>\n      <td>0.000687</td>\n      <td>70</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>123</td>\n      <td>0.000001</td>\n      <td>0.0001</td>\n      <td>1</td>\n      <td>0.847456</td>\n      <td>0.911272</td>\n      <td>0.879407</td>\n      <td>0.870021</td>\n      <td>0.314298</td>\n      <td>0.000200</td>\n      <td>0.000686</td>\n      <td>36</td>\n      <td>19</td>\n    </tr>\n  </tbody>\n</table>\n<p>74 rows  13 columns</p>\n</div>"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataframe with all best epochs for Validation accuracy\")\n",
    "best_df.sort_values(by='epoch')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with one per trials\n"
     ]
    },
    {
     "data": {
      "text/plain": "    cost_per_sample  surprisal_cost  epoch  trial   val_acc  val_updates  \\\n0          0.000001          0.0000   34.0    1.0  0.709535     0.875970   \n1          0.000001          0.0001   63.0    1.0  0.847456     0.816436   \n2          0.000001          0.0010   48.0    1.0  0.723357     0.999952   \n3          0.000001          0.0100   66.0    1.0  0.706330     0.999357   \n4          0.000001          0.1000   15.0    1.0  0.513622     0.999874   \n5          0.000010          0.0000   94.0    1.0  0.851763     0.798062   \n6          0.000010          0.0001  114.0    1.0  0.852163     0.712259   \n7          0.000010          0.0010   67.0    1.0  0.708333     0.999322   \n8          0.000010          0.0100   77.0    1.0  0.690505     0.999626   \n9          0.000010          0.1000   64.0    1.0  0.694311     0.999987   \n10         0.000100          0.0000   76.0    1.0  0.846955     0.633213   \n11         0.000100          0.0001   79.0    1.0  0.854868     0.656580   \n12         0.000100          0.0010   33.0    1.0  0.708333     0.997124   \n13         0.000100          0.0100   49.0    1.0  0.710938     0.657062   \n14         0.000100          0.1000   54.0    1.0  0.697616     0.999693   \n15         0.001000          0.0000   99.0    1.0  0.825421     0.324010   \n16         0.001000          0.0001   78.0    1.0  0.823317     0.318044   \n17         0.001000          0.0010   86.0    1.0  0.828826     0.332170   \n18         0.001000          0.0100   85.0    1.0  0.827424     0.353332   \n19         0.001000          0.1000   27.0    1.0  0.673778     0.999418   \n20         0.010000          0.0000   24.0    1.0  0.588041     0.012794   \n21         0.010000          0.0001   15.0    1.0  0.581030     0.011154   \n22         0.010000          0.0010   26.5    0.5  0.589543     0.011934   \n23         0.010000          0.0100   22.0    1.0  0.591146     0.013093   \n24         0.010000          0.1000   29.0    1.0  0.584235     0.018552   \n\n    train_acc  train_updates  entropy_loss  budget_loss  surprisal_loss  \\\n0    0.692842       0.874434      0.608738     0.000201        0.000000   \n1    0.859241       0.825813      0.333312     0.000190        0.000686   \n2    0.716747       0.999959      0.589404     0.000230        0.002779   \n3    0.697316       0.999430      0.610662     0.000230        0.032759   \n4    0.513822       0.999874      0.688635     0.000230        0.371828   \n5    0.867054       0.775913      0.326296     0.001787        0.000000   \n6    0.884882       0.678831      0.288565     0.001564        0.000685   \n7    0.706130       0.999204      0.594854     0.002302        0.004129   \n8    0.681958       0.999258      0.620628     0.002302        0.040500   \n9    0.689837       0.999886      0.612159     0.002303        0.092423   \n10   0.867722       0.607837      0.315703     0.014003        0.000000   \n11   0.865518       0.584586      0.321883     0.013467        0.000687   \n12   0.700321       0.997477      0.599425     0.022979        0.006047   \n13   0.704394       0.612886      0.603715     0.014119        0.068535   \n14   0.685029       0.999674      0.618443     0.023029        0.412860   \n15   0.840545       0.310575      0.373079     0.071547        0.000000   \n16   0.835270       0.305310      0.379761     0.070334        0.000686   \n17   0.832599       0.291738      0.387923     0.067208        0.006859   \n18   0.834535       0.312422      0.381554     0.071973        0.068578   \n19   0.664530       0.999479      0.636984     0.230250        0.303331   \n20   0.577390       0.012584      0.673652     0.028990        0.000000   \n21   0.576389       0.010682      0.676664     0.024607        0.000687   \n22   0.580729       0.011498      0.673376     0.026488        0.006866   \n23   0.582732       0.012701      0.671316     0.029260        0.068657   \n24   0.572249       0.018664      0.676222     0.042996        0.686543   \n\n    list_index  \n0         24.0  \n1         36.0  \n2         33.0  \n3         30.0  \n4         27.0  \n5          6.0  \n6         21.0  \n7         17.0  \n8         14.0  \n9         10.0  \n10        55.0  \n11        71.0  \n12        67.0  \n13        63.0  \n14        59.0  \n15         1.0  \n16        51.0  \n17        47.0  \n18        43.0  \n19        39.0  \n20         4.0  \n21        53.0  \n22        59.0  \n23        45.0  \n24        41.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cost_per_sample</th>\n      <th>surprisal_cost</th>\n      <th>epoch</th>\n      <th>trial</th>\n      <th>val_acc</th>\n      <th>val_updates</th>\n      <th>train_acc</th>\n      <th>train_updates</th>\n      <th>entropy_loss</th>\n      <th>budget_loss</th>\n      <th>surprisal_loss</th>\n      <th>list_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000001</td>\n      <td>0.0000</td>\n      <td>34.0</td>\n      <td>1.0</td>\n      <td>0.709535</td>\n      <td>0.875970</td>\n      <td>0.692842</td>\n      <td>0.874434</td>\n      <td>0.608738</td>\n      <td>0.000201</td>\n      <td>0.000000</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000001</td>\n      <td>0.0001</td>\n      <td>63.0</td>\n      <td>1.0</td>\n      <td>0.847456</td>\n      <td>0.816436</td>\n      <td>0.859241</td>\n      <td>0.825813</td>\n      <td>0.333312</td>\n      <td>0.000190</td>\n      <td>0.000686</td>\n      <td>36.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000001</td>\n      <td>0.0010</td>\n      <td>48.0</td>\n      <td>1.0</td>\n      <td>0.723357</td>\n      <td>0.999952</td>\n      <td>0.716747</td>\n      <td>0.999959</td>\n      <td>0.589404</td>\n      <td>0.000230</td>\n      <td>0.002779</td>\n      <td>33.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000001</td>\n      <td>0.0100</td>\n      <td>66.0</td>\n      <td>1.0</td>\n      <td>0.706330</td>\n      <td>0.999357</td>\n      <td>0.697316</td>\n      <td>0.999430</td>\n      <td>0.610662</td>\n      <td>0.000230</td>\n      <td>0.032759</td>\n      <td>30.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000001</td>\n      <td>0.1000</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>0.513622</td>\n      <td>0.999874</td>\n      <td>0.513822</td>\n      <td>0.999874</td>\n      <td>0.688635</td>\n      <td>0.000230</td>\n      <td>0.371828</td>\n      <td>27.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000010</td>\n      <td>0.0000</td>\n      <td>94.0</td>\n      <td>1.0</td>\n      <td>0.851763</td>\n      <td>0.798062</td>\n      <td>0.867054</td>\n      <td>0.775913</td>\n      <td>0.326296</td>\n      <td>0.001787</td>\n      <td>0.000000</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.000010</td>\n      <td>0.0001</td>\n      <td>114.0</td>\n      <td>1.0</td>\n      <td>0.852163</td>\n      <td>0.712259</td>\n      <td>0.884882</td>\n      <td>0.678831</td>\n      <td>0.288565</td>\n      <td>0.001564</td>\n      <td>0.000685</td>\n      <td>21.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.000010</td>\n      <td>0.0010</td>\n      <td>67.0</td>\n      <td>1.0</td>\n      <td>0.708333</td>\n      <td>0.999322</td>\n      <td>0.706130</td>\n      <td>0.999204</td>\n      <td>0.594854</td>\n      <td>0.002302</td>\n      <td>0.004129</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.000010</td>\n      <td>0.0100</td>\n      <td>77.0</td>\n      <td>1.0</td>\n      <td>0.690505</td>\n      <td>0.999626</td>\n      <td>0.681958</td>\n      <td>0.999258</td>\n      <td>0.620628</td>\n      <td>0.002302</td>\n      <td>0.040500</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.000010</td>\n      <td>0.1000</td>\n      <td>64.0</td>\n      <td>1.0</td>\n      <td>0.694311</td>\n      <td>0.999987</td>\n      <td>0.689837</td>\n      <td>0.999886</td>\n      <td>0.612159</td>\n      <td>0.002303</td>\n      <td>0.092423</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.000100</td>\n      <td>0.0000</td>\n      <td>76.0</td>\n      <td>1.0</td>\n      <td>0.846955</td>\n      <td>0.633213</td>\n      <td>0.867722</td>\n      <td>0.607837</td>\n      <td>0.315703</td>\n      <td>0.014003</td>\n      <td>0.000000</td>\n      <td>55.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.000100</td>\n      <td>0.0001</td>\n      <td>79.0</td>\n      <td>1.0</td>\n      <td>0.854868</td>\n      <td>0.656580</td>\n      <td>0.865518</td>\n      <td>0.584586</td>\n      <td>0.321883</td>\n      <td>0.013467</td>\n      <td>0.000687</td>\n      <td>71.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.000100</td>\n      <td>0.0010</td>\n      <td>33.0</td>\n      <td>1.0</td>\n      <td>0.708333</td>\n      <td>0.997124</td>\n      <td>0.700321</td>\n      <td>0.997477</td>\n      <td>0.599425</td>\n      <td>0.022979</td>\n      <td>0.006047</td>\n      <td>67.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.000100</td>\n      <td>0.0100</td>\n      <td>49.0</td>\n      <td>1.0</td>\n      <td>0.710938</td>\n      <td>0.657062</td>\n      <td>0.704394</td>\n      <td>0.612886</td>\n      <td>0.603715</td>\n      <td>0.014119</td>\n      <td>0.068535</td>\n      <td>63.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.000100</td>\n      <td>0.1000</td>\n      <td>54.0</td>\n      <td>1.0</td>\n      <td>0.697616</td>\n      <td>0.999693</td>\n      <td>0.685029</td>\n      <td>0.999674</td>\n      <td>0.618443</td>\n      <td>0.023029</td>\n      <td>0.412860</td>\n      <td>59.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.001000</td>\n      <td>0.0000</td>\n      <td>99.0</td>\n      <td>1.0</td>\n      <td>0.825421</td>\n      <td>0.324010</td>\n      <td>0.840545</td>\n      <td>0.310575</td>\n      <td>0.373079</td>\n      <td>0.071547</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.001000</td>\n      <td>0.0001</td>\n      <td>78.0</td>\n      <td>1.0</td>\n      <td>0.823317</td>\n      <td>0.318044</td>\n      <td>0.835270</td>\n      <td>0.305310</td>\n      <td>0.379761</td>\n      <td>0.070334</td>\n      <td>0.000686</td>\n      <td>51.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.001000</td>\n      <td>0.0010</td>\n      <td>86.0</td>\n      <td>1.0</td>\n      <td>0.828826</td>\n      <td>0.332170</td>\n      <td>0.832599</td>\n      <td>0.291738</td>\n      <td>0.387923</td>\n      <td>0.067208</td>\n      <td>0.006859</td>\n      <td>47.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.001000</td>\n      <td>0.0100</td>\n      <td>85.0</td>\n      <td>1.0</td>\n      <td>0.827424</td>\n      <td>0.353332</td>\n      <td>0.834535</td>\n      <td>0.312422</td>\n      <td>0.381554</td>\n      <td>0.071973</td>\n      <td>0.068578</td>\n      <td>43.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.001000</td>\n      <td>0.1000</td>\n      <td>27.0</td>\n      <td>1.0</td>\n      <td>0.673778</td>\n      <td>0.999418</td>\n      <td>0.664530</td>\n      <td>0.999479</td>\n      <td>0.636984</td>\n      <td>0.230250</td>\n      <td>0.303331</td>\n      <td>39.0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.010000</td>\n      <td>0.0000</td>\n      <td>24.0</td>\n      <td>1.0</td>\n      <td>0.588041</td>\n      <td>0.012794</td>\n      <td>0.577390</td>\n      <td>0.012584</td>\n      <td>0.673652</td>\n      <td>0.028990</td>\n      <td>0.000000</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.010000</td>\n      <td>0.0001</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>0.581030</td>\n      <td>0.011154</td>\n      <td>0.576389</td>\n      <td>0.010682</td>\n      <td>0.676664</td>\n      <td>0.024607</td>\n      <td>0.000687</td>\n      <td>53.0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.010000</td>\n      <td>0.0010</td>\n      <td>26.5</td>\n      <td>0.5</td>\n      <td>0.589543</td>\n      <td>0.011934</td>\n      <td>0.580729</td>\n      <td>0.011498</td>\n      <td>0.673376</td>\n      <td>0.026488</td>\n      <td>0.006866</td>\n      <td>59.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.010000</td>\n      <td>0.0100</td>\n      <td>22.0</td>\n      <td>1.0</td>\n      <td>0.591146</td>\n      <td>0.013093</td>\n      <td>0.582732</td>\n      <td>0.012701</td>\n      <td>0.671316</td>\n      <td>0.029260</td>\n      <td>0.068657</td>\n      <td>45.0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.010000</td>\n      <td>0.1000</td>\n      <td>29.0</td>\n      <td>1.0</td>\n      <td>0.584235</td>\n      <td>0.018552</td>\n      <td>0.572249</td>\n      <td>0.018664</td>\n      <td>0.676222</td>\n      <td>0.042996</td>\n      <td>0.686543</td>\n      <td>41.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataframe with one per trials\")\n",
    "# mean_df = best_df.groupby(by=[\"cost_per_sample\", \"surprisal_cost\"])\n",
    "# sorted = best_df.groupby([\"cost_per_sample\", \"surprisal_cost\"], sort=\"val_acc\")\n",
    "sorted = best_df.groupby([\"cost_per_sample\", \"surprisal_cost\"], sort=(test_val + \"_acc\"))\n",
    "# sorted = best_df.groupby([\"surprisal_cost\"], sort=\"val_acc\")\n",
    "trial_mean = sorted.median().reset_index()\n",
    "# trial_cps_mean = best_df.groupby([\"surprisal_cost\"], sort=\"val_acc\").mean().reset_index()\n",
    "trial_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['cost_per_sample', 'surprisal_cost', 'epoch', 'trial', 'val_acc',\n       'val_updates', 'train_acc', 'train_updates', 'entropy_loss',\n       'budget_loss', 'surprisal_loss', 'list_index'],\n      dtype='object')"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_mean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\core\\series.py:679: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.0 -2.0\n",
      "0 0.0\n",
      "5 0.25\n",
      "10 0.5\n",
      "15 0.75\n",
      "20 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1080x1800 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Figure for last gridsearch.\n",
    "# pd.set_option('display.float_format', '{:.2E}'.format)\n",
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "def fmt(x, pos):\n",
    "    a, b = '{:.0e}'.format(x).split('e')\n",
    "    a = int(a)\n",
    "    return r'$10^{{{}}}$'.format(a)\n",
    "\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)\n",
    "\n",
    "# trial_mean['val_acc'].corr(trial_mean['surprisal_cost'], method='pearson')\n",
    "fig, ax = plt.subplots(figsize=(15, 25))\n",
    "cax = ax.scatter(x=np.log10(trial_mean['surprisal_cost']), y=trial_mean['val_updates'], c=np.log10(trial_mean['cost_per_sample']), marker='o', s=150, cmap='viridis')\n",
    "# ax.plot(np.log10(trial_cps_mean['surprisal_cost']), (trial_cps_mean['val_updates']/2520)*100, c='orange', label=\"mean\")\n",
    "min = np.log10(trial_mean['cost_per_sample'].min())\n",
    "max = np.log10(trial_mean['cost_per_sample'].max())\n",
    "print(min, max)\n",
    "cmap = matplotlib.cm.get_cmap('viridis')\n",
    "norm = matplotlib.colors.Normalize(vmin=min, vmax=max)\n",
    "for id, row in trial_mean[trial_mean['surprisal_cost'] == 0].iterrows():\n",
    "    print(id, norm(np.log10(row['cost_per_sample'])))\n",
    "    ax.axhline(row['val_updates'], linestyle=':', color=cmap(norm(np.log10(row['cost_per_sample']))))\n",
    "plt.xticks([-4, -3, -2, -1], [0.0001, 0.001, 0.01, 0.1])\n",
    "plt.yticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], [\"0%\", \"10%\", \"20%\", \"30%\", \"40%\", \"50%\", \"60%\", \"70%\", \"80%\", \"90%\", \"100%\"])\n",
    "plt.ylim((0, 1.01))\n",
    "cbar = fig.colorbar(cax, ticks=np.log10(trial_mean['cost_per_sample']), format=ticker.FuncFormatter(fmt))\n",
    "# cbar.ax.set_yticklabels(trial_mean['cost_per_sample'].apply(lambda x: '%1.0e'%x))\n",
    "# cbar.ax.set_yticklabels(trial_mean['cost_per_sample'], format= '%.0e')\n",
    "cbar.set_label(r\"$\\lambda$\", rotation=0, labelpad=15, size=MEDIUM_SIZE)\n",
    "ax.set_xlabel(r\"$\\gamma$\")\n",
    "ax.set_ylabel(\"Words read\")\n",
    "fig.suptitle(r\"Influence of $\\lambda$, $\\gamma$ on words read\")\n",
    "plt.legend([Line2D([0],[0],linestyle=':', c=\"black\")], ['no surprisal'])\n",
    "plt.savefig(\"gridsearch_fig_updates.png\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearer visualization\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                count  acc_median  acc_best  acc_worst  \\\ncost_per_sample surprisal_cost                                           \n0.000001        0.0001              3    0.847456  0.852664   0.699519   \n0.000010        0.0000              3    0.851763  0.855268   0.845453   \n                0.0001              3    0.852163  0.858474   0.661358   \n0.000100        0.0000              3    0.846955  0.853466   0.846955   \n                0.0001              3    0.854868  0.855869   0.658053   \n0.001000        0.0000              3    0.825421  0.833133   0.804387   \n                0.0001              3    0.823317  0.829127   0.820312   \n                0.0010              3    0.828826  0.838141   0.815505   \n                0.0100              3    0.827424  0.831931   0.811599   \n\n                                 acc_std  updates_median  updates_std  \\\ncost_per_sample surprisal_cost                                          \n0.000001        0.0001          0.086954        0.816436     0.143651   \n0.000010        0.0000          0.004974        0.798062     0.050966   \n                0.0001          0.112027        0.712259     0.224252   \n0.000100        0.0000          0.003759        0.633213     0.062935   \n                0.0001          0.113921        0.656580     0.249399   \n0.001000        0.0000          0.014879        0.324010     0.000934   \n                0.0001          0.004481        0.318044     0.005791   \n                0.0010          0.011377        0.332170     0.004579   \n                0.0100          0.010678        0.353332     0.020003   \n\n                                epoch_mean  \ncost_per_sample surprisal_cost              \n0.000001        0.0001           69.000000  \n0.000010        0.0000           80.333333  \n                0.0001           82.000000  \n0.000100        0.0000           77.000000  \n                0.0001           73.000000  \n0.001000        0.0000           82.666667  \n                0.0001           82.000000  \n                0.0010           87.333333  \n                0.0100           75.666667  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>count</th>\n      <th>acc_median</th>\n      <th>acc_best</th>\n      <th>acc_worst</th>\n      <th>acc_std</th>\n      <th>updates_median</th>\n      <th>updates_std</th>\n      <th>epoch_mean</th>\n    </tr>\n    <tr>\n      <th>cost_per_sample</th>\n      <th>surprisal_cost</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0.000001</th>\n      <th>0.0001</th>\n      <td>3</td>\n      <td>0.847456</td>\n      <td>0.852664</td>\n      <td>0.699519</td>\n      <td>0.086954</td>\n      <td>0.816436</td>\n      <td>0.143651</td>\n      <td>69.000000</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">0.000010</th>\n      <th>0.0000</th>\n      <td>3</td>\n      <td>0.851763</td>\n      <td>0.855268</td>\n      <td>0.845453</td>\n      <td>0.004974</td>\n      <td>0.798062</td>\n      <td>0.050966</td>\n      <td>80.333333</td>\n    </tr>\n    <tr>\n      <th>0.0001</th>\n      <td>3</td>\n      <td>0.852163</td>\n      <td>0.858474</td>\n      <td>0.661358</td>\n      <td>0.112027</td>\n      <td>0.712259</td>\n      <td>0.224252</td>\n      <td>82.000000</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">0.000100</th>\n      <th>0.0000</th>\n      <td>3</td>\n      <td>0.846955</td>\n      <td>0.853466</td>\n      <td>0.846955</td>\n      <td>0.003759</td>\n      <td>0.633213</td>\n      <td>0.062935</td>\n      <td>77.000000</td>\n    </tr>\n    <tr>\n      <th>0.0001</th>\n      <td>3</td>\n      <td>0.854868</td>\n      <td>0.855869</td>\n      <td>0.658053</td>\n      <td>0.113921</td>\n      <td>0.656580</td>\n      <td>0.249399</td>\n      <td>73.000000</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">0.001000</th>\n      <th>0.0000</th>\n      <td>3</td>\n      <td>0.825421</td>\n      <td>0.833133</td>\n      <td>0.804387</td>\n      <td>0.014879</td>\n      <td>0.324010</td>\n      <td>0.000934</td>\n      <td>82.666667</td>\n    </tr>\n    <tr>\n      <th>0.0001</th>\n      <td>3</td>\n      <td>0.823317</td>\n      <td>0.829127</td>\n      <td>0.820312</td>\n      <td>0.004481</td>\n      <td>0.318044</td>\n      <td>0.005791</td>\n      <td>82.000000</td>\n    </tr>\n    <tr>\n      <th>0.0010</th>\n      <td>3</td>\n      <td>0.828826</td>\n      <td>0.838141</td>\n      <td>0.815505</td>\n      <td>0.011377</td>\n      <td>0.332170</td>\n      <td>0.004579</td>\n      <td>87.333333</td>\n    </tr>\n    <tr>\n      <th>0.0100</th>\n      <td>3</td>\n      <td>0.827424</td>\n      <td>0.831931</td>\n      <td>0.811599</td>\n      <td>0.010678</td>\n      <td>0.353332</td>\n      <td>0.020003</td>\n      <td>75.666667</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean_df = mean_df.max()\n",
    "# mean_df.sort_values(by='val_acc')\n",
    "print(\"Clearer visualization\")\n",
    "# , axis=1, names=[\"acc_mean\", \"acc_std\", \"updates_mean\",\n",
    "# \"updates_std\"])\n",
    "view = pd.DataFrame({'count': sorted[test_val + \"_acc\"].count(),\n",
    "                     'acc_median': sorted[test_val + \"_acc\"].median(),\n",
    "                     'acc_best': sorted[test_val + \"_acc\"].max(),\n",
    "                     'acc_worst': sorted[test_val + \"_acc\"].min(),\n",
    "                     'acc_std': sorted[test_val + \"_acc\"].std(),\n",
    "                     'updates_median': sorted[test_val + \"_updates\"].median(),\n",
    "                     'updates_std': sorted[test_val + \"_updates\"].std(),\n",
    "                     'epoch_mean': sorted.epoch.mean()})\n",
    "# view.rename(columns=[\"acc_mean\", \"acc_std\", \"updates_mean\", \"updates_std\"])\n",
    "view[view['acc_median'] > 0.75]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "# lr0001 = [csv for csv in csvs if csv['learning_rate'][0]==0.0001 and csv['hidden_units'][0]==32]\n",
    "\n",
    "# temp = bs64_best.loc[(best_df['hidden_units']==96)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n",
      "C:\\Users\\emyms\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not os.path.exists(f\"{folder}/plots\"):\n",
    "    os.makedirs(f\"{folder}/plots\")\n",
    "\n",
    "for i, df in enumerate(csvs):\n",
    "    # df.loc[:, ['val_updates', 'train_updates']] = df[['val_updates', 'train_updates']] / 2520\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "\n",
    "    a = df[['val_acc', 'train_acc']].plot(figsize= (20, 15), ax=ax1, legend=None)\n",
    "    ax1.set_ylim(0, 1.01)\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    # ax1.legend(loc=2)\n",
    "    lns = ax1.get_lines()\n",
    "    ax2 = ax1.twinx()\n",
    "    b = df['val_updates'].plot(ax=ax2, c='r', label=\"val_updates\")\n",
    "    c = df['train_updates'].plot(ax=ax2, c='g', label=\"train_updates\")\n",
    "    ax2.set_ylabel(\"Updates\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylim(0, 1.01)\n",
    "    # lns.append(ax2.get_lines)\n",
    "    # labs = [l.get_label() for l in lns]\n",
    "    fig.legend(bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\n",
    "    fig.savefig(f\"{folder}/plots/idx{i}_acc{round(best_accs[i], 2)}_cps{csvs[i].cost_per_sample[0]}_s{csvs[i].surprisal_cost[0]}_exp{csvs[i].exp[0]}.png\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# if not os.path.exists(f\"{folder}/lrplots\"):\n",
    "#     os.makedirs(f\"{folder}/lrplots\")\n",
    "#\n",
    "# for i, df in enumerate(lr0001):\n",
    "#     df[['val_acc', 'train_acc']].plot().get_figure().savefig(f\"{folder}/lrplots/idx{i}.png\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# best_hyper = pd.concat(lr0001)\n",
    "#\n",
    "# best_hyper_diff = best_hyper['val_acc'].diff(15)\n",
    "# best_hyper_diff.abs().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "# s_read_vocab = []\n",
    "# s_skipped_vocab = []\n",
    "# ns_read_vocab = []\n",
    "# ns_skipped_vocab = []\n",
    "#\n",
    "# s_read_surp = []\n",
    "# s_skipped_surp = []\n",
    "# ns_read_surp = []\n",
    "# ns_skipped_surp = []\n",
    "#\n",
    "# i=0\n",
    "# for file in os.listdir(folder + '/analysis'):\n",
    "#     file = folder + '/analysis/' + file\n",
    "#     if \"pkl\" in file:\n",
    "#         if \"SC0.01\" in file:\n",
    "#             if \"non\" in file:\n",
    "#                 s_skipped_vocab.append(pickle.load(open(file, 'rb')))\n",
    "#             else:\n",
    "#                 s_read_vocab.append(pickle.load(open(file, 'rb')))\n",
    "#         else:\n",
    "#             if \"non\" in file:\n",
    "#                 ns_skipped_vocab.append(pickle.load(open(file, 'rb')))\n",
    "#             else:\n",
    "#                 ns_read_vocab.append(pickle.load(open(file, 'rb')))\n",
    "#     else:\n",
    "#         if \"SC0.01\" in file:\n",
    "#             if \"non\" in file:\n",
    "#                 s_skipped_surp.append(np.load(open(file, 'rb')))\n",
    "#             else:\n",
    "#                 s_read_surp.append(np.load(open(file, 'rb')))\n",
    "#         else:\n",
    "#             if \"non\" in file:\n",
    "#                 ns_skipped_surp.append(np.load(open(file, 'rb')))\n",
    "#             else:\n",
    "#                 ns_read_surp.append(np.load(open(file, 'rb')))\n",
    "#     i += 1\n",
    "#\n",
    "# print(f\"{i} files were condsidered ({i/4} experiments)\")\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "# nsr_surp = ns_read_surp[0]\n",
    "# nss_surp = ns_skipped_surp[0]\n",
    "#\n",
    "# print(len(nss_surp))\n",
    "# print(len(nsr_surp))\n",
    "#\n",
    "# print(nsr_surp.mean())\n",
    "# print(nss_surp.mean())\n",
    "#\n",
    "# plt.hist(nss_surp)\n",
    "# plt.hist(nsr_surp)\n",
    "# plt.show()\n",
    "#\n",
    "#\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "# sr_surp = s_read_surp[0]\n",
    "# ss_surp = s_skipped_surp[0]\n",
    "#\n",
    "# print(f\"skipped length {len(ss_surp)}\")\n",
    "# print(f\"read length {len(sr_surp)}\")\n",
    "#\n",
    "# print(f\"read surprisal {sr_surp.mean()}\")\n",
    "# print(f\"skipped surprisal {ss_surp.mean()}\")\n",
    "#\n",
    "# plt.hist(ss_surp)\n",
    "# plt.hist(sr_surp)\n",
    "# plt.show()\n",
    "#\n",
    "# # print(ss_surp)\n",
    "#\n",
    "# print(np.count_nonzero(sr_surp == 0))\n",
    "# print(np.count_nonzero(ss_surp == 0))\n",
    "#\n",
    "# print(s_skipped_vocab)\n",
    "# print(s_read_vocab)\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}