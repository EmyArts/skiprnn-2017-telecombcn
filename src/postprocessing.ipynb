{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%% Import files\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Nans: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "folder = '../../FinalResults'\n",
    "\n",
    "csvs = []\n",
    "\n",
    "count_nan = 0\n",
    "for i, file in enumerate(os.listdir(folder + '/csvs')):\n",
    "    df = pd.read_csv(folder + '/csvs' + '/' + file)\n",
    "    if (df.batch_size == 64).any():\n",
    "        df.rename(columns={'Unnamed: 0' : 'epoch'}, inplace=True)\n",
    "        count_nan += df.shape[0] * df.shape[1] - np.sum(df.count())\n",
    "        df['list_index'] = len(csvs)\n",
    "        filename = file.split(\"_\")\n",
    "        # print(filename[0][-2:].isdigit())\n",
    "        if filename[0][-2:].isdigit(): df['exp'] = filename[0][-2:]\n",
    "        else: df['exp'] = filename[0][-1]\n",
    "        csvs.append(df)\n",
    "\n",
    "print(f\"Total number of Nans: {count_nan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "nan_index = []\n",
    "for i, df in enumerate(csvs):\n",
    "    nan_index.append(list(df[(df['val_acc'].isnull()) | (df['train_acc'].isnull()) |(df['train_updates'].isnull()) | (df['val_updates'].isnull())].index))\n",
    "\n",
    "print(nan_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# weird_acc = []\n",
    "# for i, df in enumerate(csvs):\n",
    "#     weird_acc.append(list(df[(df['val_acc']>1) | (df['val_acc']<0.1) | (df['train_acc']>1) | (df['train_acc']<0.1)].index))\n",
    "# # for df in csvs:\n",
    "# #     df.drop(df[(df['val_acc']>1) | (df['val_acc']<0)].index, inplace = True)\n",
    "#\n",
    "# see = [csvs[i].iloc[l[0]] for i, l in enumerate(weird_acc) if l]\n",
    "#\n",
    "# early_stopped_dfs = []\n",
    "# for i in range(len(csvs)):\n",
    "#     n = nan_index[i]\n",
    "#     v = weird_acc[i]\n",
    "#     if n and v:\n",
    "#         early_stopped_dfs.append(csvs[i].iloc[[min(min(v), min(n))]])\n",
    "#     elif n:\n",
    "#         early_stopped_dfs.append(csvs[i].iloc[[min(n)]])\n",
    "#     elif v:\n",
    "#         early_stopped_dfs.append(csvs[i].iloc[[min(v)]])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# early_stopped = pd.concat(early_stopped_dfs)\n",
    "# early_stopped.drop(columns=['val_acc', 'train_acc', 'val_updates', 'train_updates', 'early_stopping'], inplace=True)\n",
    "# print(\"Networks that stopped early\")\n",
    "# early_stopped\n",
    "# csvs[0].columns\n",
    "# csvs[2]\n",
    "# for df in csvs:\n",
    "#     print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# manual_early_stopping = {4: 36,5: 28, 12: 12, 14:27, 32:18, 35:23, 41:35, 46: 12,53:12}\n",
    "#\n",
    "# for key, val in manual_early_stopping.items():\n",
    "#     csvs[key].drop(labels = range(val, 40), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             epoch  batch_size  cost_per_sample  hidden_units  learning_rate  \\\ncount  1928.000000      1928.0       1928.00000        1928.0   1.928000e+03   \nmean     49.276971        64.0          0.00001          32.0   2.500000e-04   \nstd      30.374125         0.0          0.00000           0.0   1.084483e-19   \nmin       0.000000        64.0          0.00001          32.0   2.500000e-04   \n25%      24.000000        64.0          0.00001          32.0   2.500000e-04   \n50%      48.000000        64.0          0.00001          32.0   2.500000e-04   \n75%      72.000000        64.0          0.00001          32.0   2.500000e-04   \nmax     124.000000        64.0          0.00001          32.0   2.500000e-04   \n\n       surprisal_cost        trial      val_acc  val_updates    train_acc  \\\ncount     1928.000000  1928.000000  1928.000000  1928.000000  1928.000000   \nmean         0.050207     4.314834     0.810411   223.877517     0.830043   \nstd          0.050013     2.876447     0.071036    60.999247     0.083293   \nmin          0.000000     0.000000     0.487480     7.765224     0.486245   \n25%          0.000000     2.000000     0.809595   195.572170     0.826155   \n50%          0.100000     4.000000     0.835587   236.751198     0.855502   \n75%          0.100000     7.000000     0.846354   258.710724     0.873331   \nmax          0.100000     9.000000     0.858373  1057.760254     0.906851   \n\n       train_updates     test_acc  test_updates  entropy_loss  budget_loss  \\\ncount    1928.000000  1928.000000   1928.000000   1928.000000  1928.000000   \nmean      232.940126     0.806351    218.764340      0.371187     0.002329   \nstd       111.720834     0.070771     60.223974      0.101901     0.001117   \nmin         7.737981     0.489450      7.761285      0.241019     0.000077   \n25%       196.871300     0.805672    191.324699      0.306981     0.001969   \n50%       232.421036     0.831831    231.357140      0.338705     0.002324   \n75%       259.188263     0.842147    252.426167      0.392793     0.002592   \nmax      1717.352417     0.854567   1074.052002      0.698154     0.017174   \n\n        list_index   test_time  surprisal_loss  \ncount  1928.000000  968.000000    9.680000e+02  \nmean      9.127075  265.082427    3.265146e-03  \nstd       5.745234   37.457993    2.042111e-02  \nmin       0.000000  183.094056    9.910789e-08  \n25%       4.000000  250.829765    6.870996e-05  \n50%       9.000000  281.198245    1.515942e-04  \n75%      14.000000  291.086404    3.557171e-04  \nmax      19.000000  312.481397    2.285060e-01  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>batch_size</th>\n      <th>cost_per_sample</th>\n      <th>hidden_units</th>\n      <th>learning_rate</th>\n      <th>surprisal_cost</th>\n      <th>trial</th>\n      <th>val_acc</th>\n      <th>val_updates</th>\n      <th>train_acc</th>\n      <th>train_updates</th>\n      <th>test_acc</th>\n      <th>test_updates</th>\n      <th>entropy_loss</th>\n      <th>budget_loss</th>\n      <th>list_index</th>\n      <th>test_time</th>\n      <th>surprisal_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1928.000000</td>\n      <td>1928.0</td>\n      <td>1928.00000</td>\n      <td>1928.0</td>\n      <td>1.928000e+03</td>\n      <td>1928.000000</td>\n      <td>1928.000000</td>\n      <td>1928.000000</td>\n      <td>1928.000000</td>\n      <td>1928.000000</td>\n      <td>1928.000000</td>\n      <td>1928.000000</td>\n      <td>1928.000000</td>\n      <td>1928.000000</td>\n      <td>1928.000000</td>\n      <td>1928.000000</td>\n      <td>968.000000</td>\n      <td>9.680000e+02</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>49.276971</td>\n      <td>64.0</td>\n      <td>0.00001</td>\n      <td>32.0</td>\n      <td>2.500000e-04</td>\n      <td>0.050207</td>\n      <td>4.314834</td>\n      <td>0.810411</td>\n      <td>223.877517</td>\n      <td>0.830043</td>\n      <td>232.940126</td>\n      <td>0.806351</td>\n      <td>218.764340</td>\n      <td>0.371187</td>\n      <td>0.002329</td>\n      <td>9.127075</td>\n      <td>265.082427</td>\n      <td>3.265146e-03</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>30.374125</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>1.084483e-19</td>\n      <td>0.050013</td>\n      <td>2.876447</td>\n      <td>0.071036</td>\n      <td>60.999247</td>\n      <td>0.083293</td>\n      <td>111.720834</td>\n      <td>0.070771</td>\n      <td>60.223974</td>\n      <td>0.101901</td>\n      <td>0.001117</td>\n      <td>5.745234</td>\n      <td>37.457993</td>\n      <td>2.042111e-02</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>64.0</td>\n      <td>0.00001</td>\n      <td>32.0</td>\n      <td>2.500000e-04</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.487480</td>\n      <td>7.765224</td>\n      <td>0.486245</td>\n      <td>7.737981</td>\n      <td>0.489450</td>\n      <td>7.761285</td>\n      <td>0.241019</td>\n      <td>0.000077</td>\n      <td>0.000000</td>\n      <td>183.094056</td>\n      <td>9.910789e-08</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>24.000000</td>\n      <td>64.0</td>\n      <td>0.00001</td>\n      <td>32.0</td>\n      <td>2.500000e-04</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>0.809595</td>\n      <td>195.572170</td>\n      <td>0.826155</td>\n      <td>196.871300</td>\n      <td>0.805672</td>\n      <td>191.324699</td>\n      <td>0.306981</td>\n      <td>0.001969</td>\n      <td>4.000000</td>\n      <td>250.829765</td>\n      <td>6.870996e-05</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>48.000000</td>\n      <td>64.0</td>\n      <td>0.00001</td>\n      <td>32.0</td>\n      <td>2.500000e-04</td>\n      <td>0.100000</td>\n      <td>4.000000</td>\n      <td>0.835587</td>\n      <td>236.751198</td>\n      <td>0.855502</td>\n      <td>232.421036</td>\n      <td>0.831831</td>\n      <td>231.357140</td>\n      <td>0.338705</td>\n      <td>0.002324</td>\n      <td>9.000000</td>\n      <td>281.198245</td>\n      <td>1.515942e-04</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>72.000000</td>\n      <td>64.0</td>\n      <td>0.00001</td>\n      <td>32.0</td>\n      <td>2.500000e-04</td>\n      <td>0.100000</td>\n      <td>7.000000</td>\n      <td>0.846354</td>\n      <td>258.710724</td>\n      <td>0.873331</td>\n      <td>259.188263</td>\n      <td>0.842147</td>\n      <td>252.426167</td>\n      <td>0.392793</td>\n      <td>0.002592</td>\n      <td>14.000000</td>\n      <td>291.086404</td>\n      <td>3.557171e-04</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>124.000000</td>\n      <td>64.0</td>\n      <td>0.00001</td>\n      <td>32.0</td>\n      <td>2.500000e-04</td>\n      <td>0.100000</td>\n      <td>9.000000</td>\n      <td>0.858373</td>\n      <td>1057.760254</td>\n      <td>0.906851</td>\n      <td>1717.352417</td>\n      <td>0.854567</td>\n      <td>1074.052002</td>\n      <td>0.698154</td>\n      <td>0.017174</td>\n      <td>19.000000</td>\n      <td>312.481397</td>\n      <td>2.285060e-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_df = pd.concat(csvs)\n",
    "big_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x1ea92548d08>"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "big_df['val_acc'].hist(figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_rows = []\n",
    "best_accs = []\n",
    "for df in csvs:\n",
    "    best_accs.append(df.loc[:, 'test_acc'].max())\n",
    "    best_rows.append(df.loc[df.loc[:, 'test_acc'].argmax()].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_df = pd.DataFrame(best_rows)\n",
    "best_df = best_df.drop(columns= [\"learning_rate\", \"hidden_units\", \"batch_size\", \"early_stopping\"])\n",
    "\n",
    "original = best_df[best_df['surprisal_cost'] == 0]\n",
    "surprisal = best_df[best_df['surprisal_cost'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No surprisal\n"
     ]
    },
    {
     "data": {
      "text/plain": "    epoch  cost_per_sample  surprisal_cost  trial   val_acc  val_updates  \\\n0     101          0.00001             0.0      0  0.851362   189.166367   \n1      69          0.00001             0.0      4  0.853666   222.241089   \n2      96          0.00001             0.0      8  0.853265   215.021637   \n6     100          0.00001             0.0      1  0.853365   243.789871   \n7     106          0.00001             0.0      5  0.855068   240.731567   \n8      62          0.00001             0.0      9  0.849259   230.488785   \n12     90          0.00001             0.0      2  0.856871   225.564407   \n13     70          0.00001             0.0      6  0.850761   225.186005   \n16     65          0.00001             0.0      3  0.845954   199.506012   \n17     92          0.00001             0.0      7  0.848958   212.197220   \n\n    train_acc  train_updates  test_acc  test_updates  entropy_loss  \\\n0    0.878940     187.348297  0.849426    184.302612      0.291569   \n1    0.877671     213.661987  0.853432    216.965149      0.301061   \n2    0.889156     206.730164  0.852097    209.570511      0.277303   \n6    0.878272     232.360840  0.851229    238.080658      0.294586   \n7    0.895700     232.414993  0.854567    234.920212      0.259895   \n8    0.863782     219.077850  0.845820    225.571243      0.318148   \n12   0.888221     207.783051  0.851763    219.863388      0.278071   \n13   0.875467     212.926956  0.847089    220.686691      0.299569   \n16   0.868122     206.696976  0.843616    195.224564      0.313960   \n17   0.877671     214.984970  0.848357    207.086868      0.300113   \n\n    budget_loss  list_index exp  test_time  surprisal_loss  \n0      0.001873           0   0        NaN             NaN  \n1      0.002137           1   0        NaN             NaN  \n2      0.002067           2   0        NaN             NaN  \n6      0.002324           6   2        NaN             NaN  \n7      0.002324           7   2        NaN             NaN  \n8      0.002191           8   2        NaN             NaN  \n12     0.002078          12   4        NaN             NaN  \n13     0.002129          13   4        NaN             NaN  \n16     0.002067          16   6        NaN             NaN  \n17     0.002150          17   6        NaN             NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>cost_per_sample</th>\n      <th>surprisal_cost</th>\n      <th>trial</th>\n      <th>val_acc</th>\n      <th>val_updates</th>\n      <th>train_acc</th>\n      <th>train_updates</th>\n      <th>test_acc</th>\n      <th>test_updates</th>\n      <th>entropy_loss</th>\n      <th>budget_loss</th>\n      <th>list_index</th>\n      <th>exp</th>\n      <th>test_time</th>\n      <th>surprisal_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>101</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.851362</td>\n      <td>189.166367</td>\n      <td>0.878940</td>\n      <td>187.348297</td>\n      <td>0.849426</td>\n      <td>184.302612</td>\n      <td>0.291569</td>\n      <td>0.001873</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>69</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>0.853666</td>\n      <td>222.241089</td>\n      <td>0.877671</td>\n      <td>213.661987</td>\n      <td>0.853432</td>\n      <td>216.965149</td>\n      <td>0.301061</td>\n      <td>0.002137</td>\n      <td>1</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>96</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>8</td>\n      <td>0.853265</td>\n      <td>215.021637</td>\n      <td>0.889156</td>\n      <td>206.730164</td>\n      <td>0.852097</td>\n      <td>209.570511</td>\n      <td>0.277303</td>\n      <td>0.002067</td>\n      <td>2</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>100</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0.853365</td>\n      <td>243.789871</td>\n      <td>0.878272</td>\n      <td>232.360840</td>\n      <td>0.851229</td>\n      <td>238.080658</td>\n      <td>0.294586</td>\n      <td>0.002324</td>\n      <td>6</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>106</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>0.855068</td>\n      <td>240.731567</td>\n      <td>0.895700</td>\n      <td>232.414993</td>\n      <td>0.854567</td>\n      <td>234.920212</td>\n      <td>0.259895</td>\n      <td>0.002324</td>\n      <td>7</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>62</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>9</td>\n      <td>0.849259</td>\n      <td>230.488785</td>\n      <td>0.863782</td>\n      <td>219.077850</td>\n      <td>0.845820</td>\n      <td>225.571243</td>\n      <td>0.318148</td>\n      <td>0.002191</td>\n      <td>8</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>90</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0.856871</td>\n      <td>225.564407</td>\n      <td>0.888221</td>\n      <td>207.783051</td>\n      <td>0.851763</td>\n      <td>219.863388</td>\n      <td>0.278071</td>\n      <td>0.002078</td>\n      <td>12</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>70</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>0.850761</td>\n      <td>225.186005</td>\n      <td>0.875467</td>\n      <td>212.926956</td>\n      <td>0.847089</td>\n      <td>220.686691</td>\n      <td>0.299569</td>\n      <td>0.002129</td>\n      <td>13</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>65</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>0.845954</td>\n      <td>199.506012</td>\n      <td>0.868122</td>\n      <td>206.696976</td>\n      <td>0.843616</td>\n      <td>195.224564</td>\n      <td>0.313960</td>\n      <td>0.002067</td>\n      <td>16</td>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>92</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>7</td>\n      <td>0.848958</td>\n      <td>212.197220</td>\n      <td>0.877671</td>\n      <td>214.984970</td>\n      <td>0.848357</td>\n      <td>207.086868</td>\n      <td>0.300113</td>\n      <td>0.002150</td>\n      <td>17</td>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"No surprisal\")\n",
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With surprisal\n"
     ]
    },
    {
     "data": {
      "text/plain": "    epoch  cost_per_sample  surprisal_cost  trial   val_acc  val_updates  \\\n3     106          0.00001             0.1      0  0.856470   251.779846   \n4      62          0.00001             0.1      4  0.851562   259.682983   \n5      78          0.00001             0.1      8  0.856070   255.205826   \n9      63          0.00001             0.1      1  0.852764   268.748810   \n10     75          0.00001             0.1      5  0.854868   253.472458   \n11     63          0.00001             0.1      9  0.850761   253.478561   \n14     98          0.00001             0.1      2  0.854968   270.759308   \n15     96          0.00001             0.1      6  0.856170   265.819397   \n18     90          0.00001             0.1      3  0.855769   263.628693   \n19     66          0.00001             0.1      7  0.848458   254.710831   \n\n    train_acc  train_updates  test_acc  test_updates  entropy_loss  \\\n3    0.883681     251.262558  0.851496    245.572983      0.293969   \n4    0.866987     259.738770  0.846554    253.301422      0.321706   \n5    0.885550     253.961075  0.849159    248.836411      0.284384   \n9    0.862714     266.005554  0.852297    262.422546      0.326005   \n10   0.879674     252.472626  0.851629    247.125534      0.292553   \n11   0.872863     252.718887  0.849292    247.109436      0.313381   \n14   0.896368     270.525635  0.849760    264.440582      0.273090   \n15   0.888755     265.117645  0.847690    259.593018      0.284677   \n18   0.890158     262.010498  0.848558    257.223419      0.281843   \n19   0.876002     253.304886  0.846822    248.406052      0.303374   \n\n    budget_loss  list_index exp   test_time  surprisal_loss  \n3      0.002513           3   1  278.729100        0.000430  \n4      0.002597           4   1  288.127201        0.000087  \n5      0.002540           5   1  186.524801        0.000205  \n9      0.002660           9   3  278.212066        0.000039  \n10     0.002525          10   3  278.835318        0.000230  \n11     0.002527          11   3  195.183640        0.000051  \n14     0.002705          14   5  296.829465        0.000080  \n15     0.002651          15   5  275.423388        0.000465  \n18     0.002620          18   7  247.724299        0.000030  \n19     0.002533          19   7  289.292016        0.000382  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>cost_per_sample</th>\n      <th>surprisal_cost</th>\n      <th>trial</th>\n      <th>val_acc</th>\n      <th>val_updates</th>\n      <th>train_acc</th>\n      <th>train_updates</th>\n      <th>test_acc</th>\n      <th>test_updates</th>\n      <th>entropy_loss</th>\n      <th>budget_loss</th>\n      <th>list_index</th>\n      <th>exp</th>\n      <th>test_time</th>\n      <th>surprisal_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>106</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>0</td>\n      <td>0.856470</td>\n      <td>251.779846</td>\n      <td>0.883681</td>\n      <td>251.262558</td>\n      <td>0.851496</td>\n      <td>245.572983</td>\n      <td>0.293969</td>\n      <td>0.002513</td>\n      <td>3</td>\n      <td>1</td>\n      <td>278.729100</td>\n      <td>0.000430</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>62</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>4</td>\n      <td>0.851562</td>\n      <td>259.682983</td>\n      <td>0.866987</td>\n      <td>259.738770</td>\n      <td>0.846554</td>\n      <td>253.301422</td>\n      <td>0.321706</td>\n      <td>0.002597</td>\n      <td>4</td>\n      <td>1</td>\n      <td>288.127201</td>\n      <td>0.000087</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>78</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>8</td>\n      <td>0.856070</td>\n      <td>255.205826</td>\n      <td>0.885550</td>\n      <td>253.961075</td>\n      <td>0.849159</td>\n      <td>248.836411</td>\n      <td>0.284384</td>\n      <td>0.002540</td>\n      <td>5</td>\n      <td>1</td>\n      <td>186.524801</td>\n      <td>0.000205</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>63</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>1</td>\n      <td>0.852764</td>\n      <td>268.748810</td>\n      <td>0.862714</td>\n      <td>266.005554</td>\n      <td>0.852297</td>\n      <td>262.422546</td>\n      <td>0.326005</td>\n      <td>0.002660</td>\n      <td>9</td>\n      <td>3</td>\n      <td>278.212066</td>\n      <td>0.000039</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>75</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>5</td>\n      <td>0.854868</td>\n      <td>253.472458</td>\n      <td>0.879674</td>\n      <td>252.472626</td>\n      <td>0.851629</td>\n      <td>247.125534</td>\n      <td>0.292553</td>\n      <td>0.002525</td>\n      <td>10</td>\n      <td>3</td>\n      <td>278.835318</td>\n      <td>0.000230</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>63</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>9</td>\n      <td>0.850761</td>\n      <td>253.478561</td>\n      <td>0.872863</td>\n      <td>252.718887</td>\n      <td>0.849292</td>\n      <td>247.109436</td>\n      <td>0.313381</td>\n      <td>0.002527</td>\n      <td>11</td>\n      <td>3</td>\n      <td>195.183640</td>\n      <td>0.000051</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>98</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>2</td>\n      <td>0.854968</td>\n      <td>270.759308</td>\n      <td>0.896368</td>\n      <td>270.525635</td>\n      <td>0.849760</td>\n      <td>264.440582</td>\n      <td>0.273090</td>\n      <td>0.002705</td>\n      <td>14</td>\n      <td>5</td>\n      <td>296.829465</td>\n      <td>0.000080</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>96</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>6</td>\n      <td>0.856170</td>\n      <td>265.819397</td>\n      <td>0.888755</td>\n      <td>265.117645</td>\n      <td>0.847690</td>\n      <td>259.593018</td>\n      <td>0.284677</td>\n      <td>0.002651</td>\n      <td>15</td>\n      <td>5</td>\n      <td>275.423388</td>\n      <td>0.000465</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>90</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>3</td>\n      <td>0.855769</td>\n      <td>263.628693</td>\n      <td>0.890158</td>\n      <td>262.010498</td>\n      <td>0.848558</td>\n      <td>257.223419</td>\n      <td>0.281843</td>\n      <td>0.002620</td>\n      <td>18</td>\n      <td>7</td>\n      <td>247.724299</td>\n      <td>0.000030</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>66</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>7</td>\n      <td>0.848458</td>\n      <td>254.710831</td>\n      <td>0.876002</td>\n      <td>253.304886</td>\n      <td>0.846822</td>\n      <td>248.406052</td>\n      <td>0.303374</td>\n      <td>0.002533</td>\n      <td>19</td>\n      <td>7</td>\n      <td>289.292016</td>\n      <td>0.000382</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"With surprisal\")\n",
    "surprisal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with all best epochs for Validation accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": "    epoch  cost_per_sample  surprisal_cost  trial   val_acc  val_updates  \\\n8      62          0.00001             0.0      9  0.849259   230.488785   \n4      62          0.00001             0.1      4  0.851562   259.682983   \n9      63          0.00001             0.1      1  0.852764   268.748810   \n11     63          0.00001             0.1      9  0.850761   253.478561   \n16     65          0.00001             0.0      3  0.845954   199.506012   \n19     66          0.00001             0.1      7  0.848458   254.710831   \n1      69          0.00001             0.0      4  0.853666   222.241089   \n13     70          0.00001             0.0      6  0.850761   225.186005   \n10     75          0.00001             0.1      5  0.854868   253.472458   \n5      78          0.00001             0.1      8  0.856070   255.205826   \n18     90          0.00001             0.1      3  0.855769   263.628693   \n12     90          0.00001             0.0      2  0.856871   225.564407   \n17     92          0.00001             0.0      7  0.848958   212.197220   \n2      96          0.00001             0.0      8  0.853265   215.021637   \n15     96          0.00001             0.1      6  0.856170   265.819397   \n14     98          0.00001             0.1      2  0.854968   270.759308   \n6     100          0.00001             0.0      1  0.853365   243.789871   \n0     101          0.00001             0.0      0  0.851362   189.166367   \n7     106          0.00001             0.0      5  0.855068   240.731567   \n3     106          0.00001             0.1      0  0.856470   251.779846   \n\n    train_acc  train_updates  test_acc  test_updates  entropy_loss  \\\n8    0.863782     219.077850  0.845820    225.571243      0.318148   \n4    0.866987     259.738770  0.846554    253.301422      0.321706   \n9    0.862714     266.005554  0.852297    262.422546      0.326005   \n11   0.872863     252.718887  0.849292    247.109436      0.313381   \n16   0.868122     206.696976  0.843616    195.224564      0.313960   \n19   0.876002     253.304886  0.846822    248.406052      0.303374   \n1    0.877671     213.661987  0.853432    216.965149      0.301061   \n13   0.875467     212.926956  0.847089    220.686691      0.299569   \n10   0.879674     252.472626  0.851629    247.125534      0.292553   \n5    0.885550     253.961075  0.849159    248.836411      0.284384   \n18   0.890158     262.010498  0.848558    257.223419      0.281843   \n12   0.888221     207.783051  0.851763    219.863388      0.278071   \n17   0.877671     214.984970  0.848357    207.086868      0.300113   \n2    0.889156     206.730164  0.852097    209.570511      0.277303   \n15   0.888755     265.117645  0.847690    259.593018      0.284677   \n14   0.896368     270.525635  0.849760    264.440582      0.273090   \n6    0.878272     232.360840  0.851229    238.080658      0.294586   \n0    0.878940     187.348297  0.849426    184.302612      0.291569   \n7    0.895700     232.414993  0.854567    234.920212      0.259895   \n3    0.883681     251.262558  0.851496    245.572983      0.293969   \n\n    budget_loss  list_index exp   test_time  surprisal_loss  \n8      0.002191           8   2         NaN             NaN  \n4      0.002597           4   1  288.127201        0.000087  \n9      0.002660           9   3  278.212066        0.000039  \n11     0.002527          11   3  195.183640        0.000051  \n16     0.002067          16   6         NaN             NaN  \n19     0.002533          19   7  289.292016        0.000382  \n1      0.002137           1   0         NaN             NaN  \n13     0.002129          13   4         NaN             NaN  \n10     0.002525          10   3  278.835318        0.000230  \n5      0.002540           5   1  186.524801        0.000205  \n18     0.002620          18   7  247.724299        0.000030  \n12     0.002078          12   4         NaN             NaN  \n17     0.002150          17   6         NaN             NaN  \n2      0.002067           2   0         NaN             NaN  \n15     0.002651          15   5  275.423388        0.000465  \n14     0.002705          14   5  296.829465        0.000080  \n6      0.002324           6   2         NaN             NaN  \n0      0.001873           0   0         NaN             NaN  \n7      0.002324           7   2         NaN             NaN  \n3      0.002513           3   1  278.729100        0.000430  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>cost_per_sample</th>\n      <th>surprisal_cost</th>\n      <th>trial</th>\n      <th>val_acc</th>\n      <th>val_updates</th>\n      <th>train_acc</th>\n      <th>train_updates</th>\n      <th>test_acc</th>\n      <th>test_updates</th>\n      <th>entropy_loss</th>\n      <th>budget_loss</th>\n      <th>list_index</th>\n      <th>exp</th>\n      <th>test_time</th>\n      <th>surprisal_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>62</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>9</td>\n      <td>0.849259</td>\n      <td>230.488785</td>\n      <td>0.863782</td>\n      <td>219.077850</td>\n      <td>0.845820</td>\n      <td>225.571243</td>\n      <td>0.318148</td>\n      <td>0.002191</td>\n      <td>8</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>62</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>4</td>\n      <td>0.851562</td>\n      <td>259.682983</td>\n      <td>0.866987</td>\n      <td>259.738770</td>\n      <td>0.846554</td>\n      <td>253.301422</td>\n      <td>0.321706</td>\n      <td>0.002597</td>\n      <td>4</td>\n      <td>1</td>\n      <td>288.127201</td>\n      <td>0.000087</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>63</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>1</td>\n      <td>0.852764</td>\n      <td>268.748810</td>\n      <td>0.862714</td>\n      <td>266.005554</td>\n      <td>0.852297</td>\n      <td>262.422546</td>\n      <td>0.326005</td>\n      <td>0.002660</td>\n      <td>9</td>\n      <td>3</td>\n      <td>278.212066</td>\n      <td>0.000039</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>63</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>9</td>\n      <td>0.850761</td>\n      <td>253.478561</td>\n      <td>0.872863</td>\n      <td>252.718887</td>\n      <td>0.849292</td>\n      <td>247.109436</td>\n      <td>0.313381</td>\n      <td>0.002527</td>\n      <td>11</td>\n      <td>3</td>\n      <td>195.183640</td>\n      <td>0.000051</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>65</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>0.845954</td>\n      <td>199.506012</td>\n      <td>0.868122</td>\n      <td>206.696976</td>\n      <td>0.843616</td>\n      <td>195.224564</td>\n      <td>0.313960</td>\n      <td>0.002067</td>\n      <td>16</td>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>66</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>7</td>\n      <td>0.848458</td>\n      <td>254.710831</td>\n      <td>0.876002</td>\n      <td>253.304886</td>\n      <td>0.846822</td>\n      <td>248.406052</td>\n      <td>0.303374</td>\n      <td>0.002533</td>\n      <td>19</td>\n      <td>7</td>\n      <td>289.292016</td>\n      <td>0.000382</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>69</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>0.853666</td>\n      <td>222.241089</td>\n      <td>0.877671</td>\n      <td>213.661987</td>\n      <td>0.853432</td>\n      <td>216.965149</td>\n      <td>0.301061</td>\n      <td>0.002137</td>\n      <td>1</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>70</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>0.850761</td>\n      <td>225.186005</td>\n      <td>0.875467</td>\n      <td>212.926956</td>\n      <td>0.847089</td>\n      <td>220.686691</td>\n      <td>0.299569</td>\n      <td>0.002129</td>\n      <td>13</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>75</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>5</td>\n      <td>0.854868</td>\n      <td>253.472458</td>\n      <td>0.879674</td>\n      <td>252.472626</td>\n      <td>0.851629</td>\n      <td>247.125534</td>\n      <td>0.292553</td>\n      <td>0.002525</td>\n      <td>10</td>\n      <td>3</td>\n      <td>278.835318</td>\n      <td>0.000230</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>78</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>8</td>\n      <td>0.856070</td>\n      <td>255.205826</td>\n      <td>0.885550</td>\n      <td>253.961075</td>\n      <td>0.849159</td>\n      <td>248.836411</td>\n      <td>0.284384</td>\n      <td>0.002540</td>\n      <td>5</td>\n      <td>1</td>\n      <td>186.524801</td>\n      <td>0.000205</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>90</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>3</td>\n      <td>0.855769</td>\n      <td>263.628693</td>\n      <td>0.890158</td>\n      <td>262.010498</td>\n      <td>0.848558</td>\n      <td>257.223419</td>\n      <td>0.281843</td>\n      <td>0.002620</td>\n      <td>18</td>\n      <td>7</td>\n      <td>247.724299</td>\n      <td>0.000030</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>90</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0.856871</td>\n      <td>225.564407</td>\n      <td>0.888221</td>\n      <td>207.783051</td>\n      <td>0.851763</td>\n      <td>219.863388</td>\n      <td>0.278071</td>\n      <td>0.002078</td>\n      <td>12</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>92</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>7</td>\n      <td>0.848958</td>\n      <td>212.197220</td>\n      <td>0.877671</td>\n      <td>214.984970</td>\n      <td>0.848357</td>\n      <td>207.086868</td>\n      <td>0.300113</td>\n      <td>0.002150</td>\n      <td>17</td>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>96</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>8</td>\n      <td>0.853265</td>\n      <td>215.021637</td>\n      <td>0.889156</td>\n      <td>206.730164</td>\n      <td>0.852097</td>\n      <td>209.570511</td>\n      <td>0.277303</td>\n      <td>0.002067</td>\n      <td>2</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>96</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>6</td>\n      <td>0.856170</td>\n      <td>265.819397</td>\n      <td>0.888755</td>\n      <td>265.117645</td>\n      <td>0.847690</td>\n      <td>259.593018</td>\n      <td>0.284677</td>\n      <td>0.002651</td>\n      <td>15</td>\n      <td>5</td>\n      <td>275.423388</td>\n      <td>0.000465</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>98</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>2</td>\n      <td>0.854968</td>\n      <td>270.759308</td>\n      <td>0.896368</td>\n      <td>270.525635</td>\n      <td>0.849760</td>\n      <td>264.440582</td>\n      <td>0.273090</td>\n      <td>0.002705</td>\n      <td>14</td>\n      <td>5</td>\n      <td>296.829465</td>\n      <td>0.000080</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>100</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0.853365</td>\n      <td>243.789871</td>\n      <td>0.878272</td>\n      <td>232.360840</td>\n      <td>0.851229</td>\n      <td>238.080658</td>\n      <td>0.294586</td>\n      <td>0.002324</td>\n      <td>6</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>101</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.851362</td>\n      <td>189.166367</td>\n      <td>0.878940</td>\n      <td>187.348297</td>\n      <td>0.849426</td>\n      <td>184.302612</td>\n      <td>0.291569</td>\n      <td>0.001873</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>106</td>\n      <td>0.00001</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>0.855068</td>\n      <td>240.731567</td>\n      <td>0.895700</td>\n      <td>232.414993</td>\n      <td>0.854567</td>\n      <td>234.920212</td>\n      <td>0.259895</td>\n      <td>0.002324</td>\n      <td>7</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>106</td>\n      <td>0.00001</td>\n      <td>0.1</td>\n      <td>0</td>\n      <td>0.856470</td>\n      <td>251.779846</td>\n      <td>0.883681</td>\n      <td>251.262558</td>\n      <td>0.851496</td>\n      <td>245.572983</td>\n      <td>0.293969</td>\n      <td>0.002513</td>\n      <td>3</td>\n      <td>1</td>\n      <td>278.729100</td>\n      <td>0.000430</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataframe with all best epochs for Validation accuracy\")\n",
    "best_df.sort_values(by='epoch')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with one per trials\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                epoch  trial   val_acc  val_updates  \\\ncost_per_sample surprisal_cost                                        \n0.00001         0.0               101      0  0.851362   189.166367   \n                0.1               106      0  0.856470   251.779846   \n\n                                train_acc  train_updates  test_acc  \\\ncost_per_sample surprisal_cost                                       \n0.00001         0.0              0.878940     187.348297  0.849426   \n                0.1              0.883681     251.262558  0.851496   \n\n                                test_updates  entropy_loss  budget_loss  \\\ncost_per_sample surprisal_cost                                            \n0.00001         0.0               184.302612      0.291569     0.001873   \n                0.1               245.572983      0.293969     0.002513   \n\n                                list_index exp  test_time  surprisal_loss  \ncost_per_sample surprisal_cost                                             \n0.00001         0.0                      0   0        NaN             NaN  \n                0.1                      3   1   278.7291         0.00043  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>epoch</th>\n      <th>trial</th>\n      <th>val_acc</th>\n      <th>val_updates</th>\n      <th>train_acc</th>\n      <th>train_updates</th>\n      <th>test_acc</th>\n      <th>test_updates</th>\n      <th>entropy_loss</th>\n      <th>budget_loss</th>\n      <th>list_index</th>\n      <th>exp</th>\n      <th>test_time</th>\n      <th>surprisal_loss</th>\n    </tr>\n    <tr>\n      <th>cost_per_sample</th>\n      <th>surprisal_cost</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">0.00001</th>\n      <th>0.0</th>\n      <td>101</td>\n      <td>0</td>\n      <td>0.851362</td>\n      <td>189.166367</td>\n      <td>0.878940</td>\n      <td>187.348297</td>\n      <td>0.849426</td>\n      <td>184.302612</td>\n      <td>0.291569</td>\n      <td>0.001873</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0.1</th>\n      <td>106</td>\n      <td>0</td>\n      <td>0.856470</td>\n      <td>251.779846</td>\n      <td>0.883681</td>\n      <td>251.262558</td>\n      <td>0.851496</td>\n      <td>245.572983</td>\n      <td>0.293969</td>\n      <td>0.002513</td>\n      <td>3</td>\n      <td>1</td>\n      <td>278.7291</td>\n      <td>0.00043</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataframe with one per trials\")\n",
    "# mean_df = best_df.groupby(by=[\"cost_per_sample\", \"surprisal_cost\"])\n",
    "sorted = best_df.groupby([\"cost_per_sample\", \"surprisal_cost\"], sort=\"test_acc\")\n",
    "sorted.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearer visualization\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                count  acc_mean  acc_best  acc_worst  \\\ncost_per_sample surprisal_cost                                         \n0.00001         0.0                10  0.849740  0.854567   0.843616   \n                0.1                10  0.849326  0.852297   0.846554   \n\n                                 acc_std  updates_mean  updates_std  \\\ncost_per_sample surprisal_cost                                        \n0.00001         0.0             0.003507     215.22719    16.753487   \n                0.1             0.002008     253.40314     7.009918   \n\n                                epoch_mean  \ncost_per_sample surprisal_cost              \n0.00001         0.0                   85.1  \n                0.1                   79.7  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>count</th>\n      <th>acc_mean</th>\n      <th>acc_best</th>\n      <th>acc_worst</th>\n      <th>acc_std</th>\n      <th>updates_mean</th>\n      <th>updates_std</th>\n      <th>epoch_mean</th>\n    </tr>\n    <tr>\n      <th>cost_per_sample</th>\n      <th>surprisal_cost</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">0.00001</th>\n      <th>0.0</th>\n      <td>10</td>\n      <td>0.849740</td>\n      <td>0.854567</td>\n      <td>0.843616</td>\n      <td>0.003507</td>\n      <td>215.22719</td>\n      <td>16.753487</td>\n      <td>85.1</td>\n    </tr>\n    <tr>\n      <th>0.1</th>\n      <td>10</td>\n      <td>0.849326</td>\n      <td>0.852297</td>\n      <td>0.846554</td>\n      <td>0.002008</td>\n      <td>253.40314</td>\n      <td>7.009918</td>\n      <td>79.7</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean_df = mean_df.max()\n",
    "# mean_df.sort_values(by='val_acc')\n",
    "print(\"Clearer visualization\")\n",
    "# , axis=1, names=[\"acc_mean\", \"acc_std\", \"updates_mean\",\n",
    "# \"updates_std\"])\n",
    "view = pd.DataFrame({'count': sorted.val_acc.count(),\n",
    "                     'acc_mean': sorted.test_acc.mean(),\n",
    "                     'acc_best': sorted.test_acc.max(),\n",
    "                     'acc_worst': sorted.test_acc.min(),\n",
    "                     'acc_std': sorted.test_acc.std(),\n",
    "                     'updates_mean': sorted.test_updates.mean(),\n",
    "                     'updates_std': sorted.test_updates.std(),\n",
    "                     'epoch_mean': sorted.epoch.mean()})\n",
    "# view.rename(columns=[\"acc_mean\", \"acc_std\", \"updates_mean\", \"updates_std\"])\n",
    "view[view['acc_mean'] > 0.835]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "lr0001 = [csv for csv in csvs if csv['learning_rate'][0]==0.0001 and csv['hidden_units'][0]==32]\n",
    "\n",
    "# temp = bs64_best.loc[(best_df['hidden_units']==96)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1080 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not os.path.exists(f\"{folder}/plots\"):\n",
    "    os.makedirs(f\"{folder}/plots\")\n",
    "\n",
    "for i, df in enumerate(csvs):\n",
    "    df.loc[:, ['val_updates', 'train_updates']] = df[['val_updates', 'train_updates']] / 2520\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "\n",
    "    a = df[['val_acc', 'train_acc']].plot(figsize= (20, 15), ax=ax1, legend=None)\n",
    "    ax1.set_ylim(0.4, 1)\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    # ax1.legend(loc=2)\n",
    "    lns = ax1.get_lines()\n",
    "    ax2 = ax1.twinx()\n",
    "    b = df['val_updates'].plot(ax=ax2, c='r', label=\"val_updates\")\n",
    "    c = df['train_updates'].plot(ax=ax2, c='g', label=\"train_updates\")\n",
    "    ax2.set_ylabel(\"Updates\")\n",
    "    ax2.set_ylim(0, 0.5)\n",
    "    # lns.append(ax2.get_lines)\n",
    "    # labs = [l.get_label() for l in lns]\n",
    "    fig.legend(bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\n",
    "    fig.savefig(f\"{folder}/plots/idx{i}_acc{round(best_accs[i], 2)}_cps{csvs[i].cost_per_sample[0]}_s{csvs[i].surprisal_cost[0]}_exp{csvs[i].exp[0]}.png\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# if not os.path.exists(f\"{folder}/lrplots\"):\n",
    "#     os.makedirs(f\"{folder}/lrplots\")\n",
    "#\n",
    "# for i, df in enumerate(lr0001):\n",
    "#     df[['val_acc', 'train_acc']].plot().get_figure().savefig(f\"{folder}/lrplots/idx{i}.png\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-69-480a54a8c449>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mbest_hyper\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlr0001\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mbest_hyper_diff\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbest_hyper\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'val_acc'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdiff\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m15\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mbest_hyper_diff\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mabs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001B[0m in \u001B[0;36mconcat\u001B[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001B[0m\n\u001B[0;32m    279\u001B[0m         \u001B[0mverify_integrity\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mverify_integrity\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    280\u001B[0m         \u001B[0mcopy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 281\u001B[1;33m         \u001B[0msort\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msort\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    282\u001B[0m     )\n\u001B[0;32m    283\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\skiprnn-2017-telecombcn\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001B[0m\n\u001B[0;32m    327\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    328\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobjs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 329\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"No objects to concatenate\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    330\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    331\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mkeys\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# best_hyper = pd.concat(lr0001)\n",
    "#\n",
    "# best_hyper_diff = best_hyper['val_acc'].diff(15)\n",
    "# best_hyper_diff.abs().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "s_read_vocab = []\n",
    "s_skipped_vocab = []\n",
    "ns_read_vocab = []\n",
    "ns_skipped_vocab = []\n",
    "\n",
    "s_read_surp = []\n",
    "s_skipped_surp = []\n",
    "ns_read_surp = []\n",
    "ns_skipped_surp = []\n",
    "\n",
    "for file in os.listdir(folder + '/analysis'):\n",
    "    file = folder + '/analysis/' + file\n",
    "    if \"pkl\" in file:\n",
    "        if \"SC0.1\" in file:\n",
    "            if \"non\" in file:\n",
    "                s_skipped_vocab.append(pickle.load(open(file, 'rb')))\n",
    "            else:\n",
    "                s_read_vocab.append(pickle.load(open(file, 'rb')))\n",
    "        else:\n",
    "            if \"non\" in file:\n",
    "                ns_skipped_vocab.append(pickle.load(open(file, 'rb')))\n",
    "            else:\n",
    "                ns_read_vocab.append(pickle.load(open(file, 'rb')))\n",
    "    else:\n",
    "        if \"SC0.1\" in file:\n",
    "            if \"non\" in file:\n",
    "                s_skipped_surp.append(np.load(open(file, 'rb')))\n",
    "            else:\n",
    "                s_read_surp.append(np.load(open(file, 'rb')))\n",
    "        else:\n",
    "            if \"non\" in file:\n",
    "                ns_skipped_surp.append(np.load(open(file, 'rb')))\n",
    "            else:\n",
    "                ns_read_surp.append(np.load(open(file, 'rb')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "6.371419091365244\n",
      "6.440586187215992\n"
     ]
    },
    {
     "data": {
      "text/plain": "(array([2239680., 2196273., 3392011., 1440654., 1335873., 2015397.,\n         630097.,  609973.,  125281.,   77299.]),\n array([ 2. ,  3.4,  4.8,  6.2,  7.6,  9. , 10.4, 11.8, 13.2, 14.6, 16. ]),\n <a list of 10 Patch objects>)"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nsr_surp = np.concatenate(ns_read_surp)\n",
    "nss_surp = np.concatenate(ns_skipped_surp)\n",
    "\n",
    "print(len(ns_skipped_surp))\n",
    "print(len(ns_read_surp))\n",
    "\n",
    "print(nsr_surp.mean())\n",
    "print(nss_surp.mean())\n",
    "\n",
    "plt.hist(nsr_surp)\n",
    "plt.hist(nss_surp)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "6.375040799040284\n",
      "6.458624541776945\n"
     ]
    },
    {
     "data": {
      "text/plain": "(array([1615240., 1567456., 2437803., 1035058.,  954253., 1459006.,\n         462115.,  450166.,   93457.,   57793.]),\n array([ 2. ,  3.4,  4.8,  6.2,  7.6,  9. , 10.4, 11.8, 13.2, 14.6, 16. ]),\n <a list of 10 Patch objects>)"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sr_surp = np.concatenate(s_read_surp)\n",
    "ss_surp = np.concatenate(s_skipped_surp)\n",
    "\n",
    "print(len(s_skipped_surp))\n",
    "print(len(s_read_surp))\n",
    "\n",
    "print(sr_surp.mean())\n",
    "print(ss_surp.mean())\n",
    "\n",
    "plt.hist(sr_surp)\n",
    "plt.hist(ss_surp)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}